{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeae1a1e",
   "metadata": {},
   "source": [
    "This file was made to calculate the precision of Contextual Predictive Mutation Testing in its original form.\n",
    "\n",
    "More in-depth discussion is in Adam Abdalla's thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import pickle\n",
    "from os import path\n",
    "import os\n",
    "from random import sample, seed\n",
    "import csv\n",
    "import pandas as pd\n",
    "import kmeans1d\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: tutorialspoint.com/file-searching-using-python\n",
    "def find_files(filename, search_path):\n",
    "   result = []\n",
    "\n",
    "    # Walking top-down from the root\n",
    "   for root, _, files in os.walk(search_path):\n",
    "      if filename in files:\n",
    "         result.append(os.path.join(root, filename))\n",
    "   return result\n",
    "\n",
    "\n",
    "# Gets characteristics and result status data from project file\n",
    "# created by pitest clustering plugin.\n",
    "def getProjectDfs(project: str) -> tuple[pd.DataFrame, pd.DataFrame] | int:\n",
    "    csv_path = \"projects/\" + project\n",
    "    charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "\n",
    "    if charPath:\n",
    "        charPath = charPath[0]\n",
    "        data = pd.read_csv(charPath,\n",
    "                            names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                    \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                    \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                            skiprows=1)\n",
    "        killedPath = find_files(\"killed.csv\", csv_path)\n",
    "\n",
    "        if killedPath:\n",
    "            killedPath = killedPath[0]\n",
    "            results = pd.read_csv(killedPath,\n",
    "                                    names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                    skiprows=1)\n",
    "            results = results.drop(columns=[\"numTests\"])\n",
    "        else:\n",
    "            print(\"Could not find killed.csv for project: \" + project)\n",
    "            return -1\n",
    "    else:\n",
    "        print(\"Could not find characteristics.csv for project: \" + project)\n",
    "        return -1\n",
    "\n",
    "    return data, results\n",
    "\n",
    "\n",
    "# Uses label encoding and clustering to change numerical data to categorical.\n",
    "# Also merges data with results.\n",
    "def dfToCategorical(data: pd.DataFrame, results: pd.DataFrame, parameters: list) -> pd.DataFrame:\n",
    "    idReduction, localityReduction, n_localVarsClusters = parameters\n",
    "    # define ordinal encoding\n",
    "    encoder = LabelEncoder()\n",
    "    newData = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"localVarsCount\",\n",
    "                    \"isInTryCatch\", \"className\", \"methodName\", \"lineNumber\"]]\n",
    "    for col in [\"mutOperator\", \"returnType\", \"className\", \"methodName\", \"id\"]:\n",
    "        newData[col] = encoder.fit_transform(newData[col])\n",
    "\n",
    "    idClustering = kmeans1d.cluster(np.asarray(newData[[\"id\"]], dtype=\"int64\"), int(math.ceil(len(data) * idReduction)))[0]\n",
    "\n",
    "    # Categorical locality variable creation.\n",
    "    newData[\"className\"] = newData[\"className\"].apply(lambda x: x*100000)\n",
    "    newData[\"methodName\"] = newData[\"methodName\"].apply(lambda x: x*1000)\n",
    "    localityClustering = kmeans1d.cluster(np.asarray(newData[[\"className\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"methodName\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"lineNumber\"]], dtype=\"int64\"), int(math.ceil(len(data) * localityReduction)))[0]\n",
    "\n",
    "    varsClustering = kmeans1d.cluster(np.asarray(newData[[\"localVarsCount\"]], dtype=\"int64\"), n_localVarsClusters)[0]\n",
    "\n",
    "    training = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"isInTryCatch\"]]\n",
    "    training[\"idCluster_id\"] = idClustering\n",
    "    training[\"localityCluster_id\"] = localityClustering\n",
    "    training[\"varsCluster_id\"] = varsClustering\n",
    "    training = training.merge(results, how=\"inner\", on=\"id\")\n",
    "\n",
    "    return training\n",
    "\n",
    "\n",
    "# Selects the sample of mutants that would be executed by CPMT in practice.\n",
    "# Does so by selecting a random, parameter-specified number of mutants per characteristic.\n",
    "def sampleSelector(fullDF: pd.DataFrame, parameters: list) -> list:\n",
    "    perOperator, perOpcode, perReturn, perTryCatch, perIDCluster, perLocCluster, perVarsCluster = parameters\n",
    "    mutantsSample = []\n",
    "    for idCluster_id in list(dict.fromkeys(fullDF[\"idCluster_id\"].tolist())):\n",
    "        idList = fullDF[fullDF[\"idCluster_id\"] ==  idCluster_id][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(idList, min(len(idList), perIDCluster)))\n",
    "    for mutOperator in list(dict.fromkeys(fullDF[\"mutOperator\"].tolist())):\n",
    "        mutList = fullDF[fullDF[\"mutOperator\"] ==  mutOperator][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(mutList, min(len(mutList), perOperator)))\n",
    "    for opcode in list(dict.fromkeys(fullDF[\"opcode\"].tolist())):\n",
    "        opList = fullDF[fullDF[\"mutOperator\"] ==  mutOperator][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(opList, min(len(opList), perOpcode)))\n",
    "    for returnType in list(dict.fromkeys(fullDF[\"returnType\"].tolist())):\n",
    "        rTypeList = fullDF[fullDF[\"returnType\"] ==  returnType][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(rTypeList, min(len(rTypeList), perReturn)))\n",
    "    notInTCList = fullDF[fullDF[\"isInTryCatch\"] == 0][\"id\"].tolist()\n",
    "    mutantsSample.extend(sample(notInTCList, min(len(notInTCList), perTryCatch)))\n",
    "    inTCList = fullDF[fullDF[\"isInTryCatch\"] == 1][\"id\"].tolist()\n",
    "    mutantsSample.extend(sample(inTCList, min(len(inTCList), perTryCatch)))\n",
    "    for localityCluster_id in list(dict.fromkeys(fullDF[\"localityCluster_id\"].tolist())):\n",
    "        locClusterList = fullDF[fullDF[\"localityCluster_id\"] == localityCluster_id][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(locClusterList, min(len(locClusterList), perLocCluster)))\n",
    "    for varsCluster_id in list(dict.fromkeys(fullDF[\"varsCluster_id\"].tolist())):\n",
    "        varsClusterList = fullDF[fullDF[\"varsCluster_id\"] == varsCluster_id][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(varsClusterList, min(len(varsClusterList), perVarsCluster)))\n",
    "\n",
    "    return mutantsSample\n",
    "\n",
    "\n",
    "# For every possible value of every characteristic, takes all mutants with that value in the sampleDF.\n",
    "# Calculates the percentage of those mutants which are killed, aka pk-score.\n",
    "# Overwrites the value in the fullDF with the pk-score. Returns new fullDF.\n",
    "def  pkScoreTransformer(fullDF: pd.DataFrame, sampleDF: pd.DataFrame) -> pd.DataFrame:\n",
    "    for idCluster_id in list(dict.fromkeys(sampleDF[\"idCluster_id\"].tolist())):\n",
    "        percentageKilled = len(sampleDF[(sampleDF[\"idCluster_id\"] == idCluster_id) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"idCluster_id\"] == idCluster_id])\n",
    "        fullDF.loc[fullDF[\"idCluster_id\"] == idCluster_id, \"idCluster_id\"] = percentageKilled\n",
    "    for mutOperator in list(dict.fromkeys(sampleDF[\"mutOperator\"].tolist())):\n",
    "        percentageKilled = len(sampleDF[(sampleDF[\"mutOperator\"] == mutOperator) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"mutOperator\"] == mutOperator])\n",
    "        fullDF.loc[fullDF[\"mutOperator\"] == mutOperator, \"mutOperator\"] = percentageKilled\n",
    "    for opcode in list(dict.fromkeys(sampleDF[\"opcode\"].tolist())):\n",
    "        percentageKilled = len(sampleDF[(sampleDF[\"opcode\"] == opcode) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"opcode\"] == opcode])\n",
    "        fullDF.loc[fullDF[\"opcode\"] == opcode, \"opcode\"] = percentageKilled\n",
    "    for returnType in list(dict.fromkeys(sampleDF[\"returnType\"].tolist())):\n",
    "        percentageKilled = len(sampleDF[(sampleDF[\"returnType\"] == returnType) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"returnType\"] == returnType])\n",
    "        fullDF.loc[fullDF[\"returnType\"] == returnType, \"returnType\"] = percentageKilled\n",
    "    percentageKilled = len(sampleDF[(sampleDF[\"isInTryCatch\"] == 0) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"isInTryCatch\"] == 0])\n",
    "    fullDF.loc[fullDF[\"isInTryCatch\"] == 0, \"isInTryCatch\"] = percentageKilled\n",
    "    percentageKilled = len(sampleDF[(sampleDF[\"isInTryCatch\"] == 1) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"isInTryCatch\"] == 1])\n",
    "    fullDF.loc[fullDF[\"isInTryCatch\"] == 1, \"isInTryCatch\"] = percentageKilled\n",
    "    for localityCluster_id in list(dict.fromkeys(fullDF[\"localityCluster_id\"].tolist())):\n",
    "        percentageKilled = len(sampleDF[(sampleDF[\"localityCluster_id\"] == localityCluster_id) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"localityCluster_id\"] == localityCluster_id])\n",
    "        fullDF.loc[fullDF[\"localityCluster_id\"] == localityCluster_id, \"localityCluster_id\"] = percentageKilled\n",
    "    for varsCluster_id in list(dict.fromkeys(fullDF[\"varsCluster_id\"].tolist())):\n",
    "        percentageKilled = len(sampleDF[(sampleDF[\"varsCluster_id\"] == varsCluster_id) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[\"varsCluster_id\"] == varsCluster_id])\n",
    "        fullDF.loc[fullDF[\"varsCluster_id\"] == varsCluster_id, \"varsCluster_id\"] = percentageKilled\n",
    "\n",
    "    return fullDF\n",
    "\n",
    "\n",
    "# Takes a list of projects, transforms their characteristic data based on parameters\n",
    "# into a single CPMT-ready dataframe. Trains the given classifier on the dataframe.\n",
    "def trainAlgorithm(projects: list, classifier: any, parameters: list) -> any:\n",
    "    samplingStop = parameters[-1]\n",
    "    classificationTraining = []\n",
    "    for project in projects:\n",
    "        print(\"Starting training project: \" + project)\n",
    "        dataframes = getProjectDfs(project)\n",
    "        if dataframes == -1:\n",
    "            continue\n",
    "        data, results = dataframes\n",
    "\n",
    "        training = dfToCategorical(data, results, parameters[:3])\n",
    "        print(\"Starting creation of classification training set for project: \" + project)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # For every project, a CPMT-ready dataframe is inserted\n",
    "        # multiple times to produce subsampling. samplingStop controls how many.\n",
    "        allMutants = []\n",
    "        mutantsSampleDfs = []\n",
    "        i = 0\n",
    "        while True:\n",
    "            print(\"iteration = \", i)\n",
    "            i += 1\n",
    "            print(samplingStop * len(training[\"id\"].tolist()), len(allMutants))\n",
    "\n",
    "            mutantsSample = sampleSelector(training, parameters[3:-1])\n",
    "            allMutants.extend(mutantsSample)\n",
    "\n",
    "            trainingSample = training[training[\"id\"].isin(mutantsSample)]\n",
    "            trainingInsert = training.copy()\n",
    "            trainingInsert = pkScoreTransformer(trainingInsert, trainingSample)\n",
    "\n",
    "            mutantsSampleDfs.append(trainingInsert)\n",
    "\n",
    "            # Basing it on \"allMutants\" means lower reductions -> more subsampling.\n",
    "            # Might want to experiment with different subsampling if classifier gets more complex.\n",
    "            if samplingStop * len(training[\"id\"].tolist()) < len(allMutants):\n",
    "                break\n",
    "\n",
    "        # All dataframes of 1 project get combined.\n",
    "        classificationTraining.append(pd.concat(mutantsSampleDfs).drop(columns=[\"id\"]))\n",
    "\n",
    "    print(\"Done with getting all classification training data.\")\n",
    "    print(time.time() - start_time)\n",
    "\n",
    "    # Dataframes of all projects get combined.\n",
    "    classificationTrainingDf = pd.concat(classificationTraining)\n",
    "    X_train = classificationTrainingDf.drop(columns=[\"killed\"]).values.tolist()\n",
    "    y_train = classificationTrainingDf[\"killed\"].tolist()\n",
    "\n",
    "    print(\"Starting training of classifier.\")\n",
    "    start_time = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(time.time() - start_time)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Predicts the result of every mutant in the given project using CPMT.\n",
    "def own_predict(classifier: any, project: str, parameters: list, useSampled: bool=False) -> tuple[np.ndarray, float]:\n",
    "    samplingStop = parameters[-1]\n",
    "    print(\"Starting prediction project: \" + project)\n",
    "    start_time = time.time()\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "\n",
    "    # Transformation from raw characteristic data to CPMT-ready data.\n",
    "    fullDF = dfToCategorical(data, results, parameters[:3])\n",
    "    sampleMutantsIDs = sampleSelector(fullDF, parameters[3:-1])\n",
    "    sampleDF = fullDF[fullDF[\"id\"].isin(sampleMutantsIDs)]\n",
    "    reduction = len(sampleMutantsIDs)/len(fullDF)\n",
    "    print(\"reduction = \" + str(reduction))\n",
    "    fullDF = pkScoreTransformer(fullDF, sampleDF)\n",
    "\n",
    "    # Saves the results of the \"executed\" mutants to overwrite the predictions.\n",
    "    if useSampled:\n",
    "        killedIndices = fullDF.index[(fullDF[\"killed\"] == 1) & (fullDF[\"id\"].isin(sampleMutantsIDs))].tolist()\n",
    "        survivedIndices = fullDF.index[(fullDF[\"killed\"] == 0) & (fullDF[\"id\"].isin(sampleMutantsIDs))].tolist()\n",
    "    fullDF = fullDF.drop(columns=[\"id\", \"killed\"])\n",
    "\n",
    "    predictResults = classifier.predict(fullDF)\n",
    "\n",
    "    # For the executed mutants, CPMT would not need to predict their result.\n",
    "    if useSampled:\n",
    "        predictResults[killedIndices] = 1\n",
    "        predictResults[survivedIndices] = 0\n",
    "    print(\"Prediction for project \" + project + \" took: \" + str(time.time() - start_time) + \" seconds.\")\n",
    "    return predictResults, reduction\n",
    "\n",
    "\n",
    "# Calculates the precision: percentage of correct predictions in decimals.\n",
    "def precisionCalc(project: str, predictions: np.ndarray) -> float:\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "    data[\"prediction\"] = predictions\n",
    "    newData = data[[\"id\", \"prediction\"]]\n",
    "    merged = newData.merge(results, how=\"inner\", on=\"id\")\n",
    "    merged.to_csv(\"projects/\" + project + \"/predictions.csv\", sep=\",\", index=False)\n",
    "    correctList = [1 if i == j else 0 for i, j in zip(merged[\"killed\"].tolist(), merged[\"prediction\"].tolist())]\n",
    "    precision = len([1 for val in correctList if val == 1 ])/len(correctList)\n",
    "    print(\"Precision = \" + str(precision))\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371c06b7",
   "metadata": {},
   "source": [
    "Parameter cell down below. Note that these are not the only parameters you might want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "     90957, 52113, 20428, 26482, 56340, 31018, 32067, 13067, 8339, 49008, 125894, 68282, 66304, 16389, 14706, 91254, 49890,\n",
    "    86054, 55284, 77324, 36147, 13506, 73920, 80157, 43981, 75358, 33399, 56134,\n",
    "    13388, 81617, ]\n",
    "# trainingProjects = [\"google-auto-service\", \"scribejava-core\", \"commons-cli\",\n",
    "#                     \"google-auto-value\",\"gson\", \"commons-io\", \"commons-codec\"]\n",
    "projects = [\"commons-text\", \"commons-codec\", \"google-auto-common\", \"scribejava-core\", \"google-auto-factory\", \"commons-csv\",\n",
    "                \"commons-cli\", \"google-auto-value\", \"gson\", \"commons-io\", ]\n",
    "parameters = [0.05, 5, 3, 3, 3, 3, 2, 2, 2]\n",
    "classifier = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23077e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8b29496",
   "metadata": {},
   "source": [
    "We used this cell for our hyperopt runs. The variables you might want to change here are those inside the space, max_evals and the reductions(in the cell above).\n",
    "You may also want to work with less seeds/projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(args):\n",
    "#     parameters = [args[\"idReduction\"], args[\"localityReduction\"], args[\"n_localVarsClusters\"], args[\"perOperator\"], args[\"perOpcode\"], args[\"perReturn\"],\n",
    "#                   args[\"perTryCatch\"], args[\"perIDCluster\"], args[\"perLocCluster\"], args[\"perVarsCluster\"], args[\"samplingStop\"]]\n",
    "#     print(args)\n",
    "#     precisions = []\n",
    "#     reductions = []\n",
    "#     for i in seeds:\n",
    "#         seed(i)\n",
    "#         classAlgo = LinearSVC(dual=False, tol=args[\"tol\"], C=args[\"c\"])\n",
    "#         classifier = trainAlgorithm(trainingProjects, classAlgo, parameters=parameters)\n",
    "#         for project in projects:\n",
    "#             results, reduction = own_predict(classifier, project, parameters=parameters)\n",
    "#             precisions.append(precisionCalc(project, results))\n",
    "#             reductions.append(reduction)\n",
    "# \n",
    "#     # If a reduction passes one of the if-statements,\n",
    "#     # we negate it with the corresponding Cluster Accuracy\n",
    "#     # reported by Mouissie to make it a little more fair.\n",
    "#     # remove reduction if-statements that you don't want.\n",
    "#     if np.mean(reductions) < 0.11:\n",
    "#         return 0.7982 - np.mean(precisions)\n",
    "#     elif np.mean(reductions) < 0.26:\n",
    "#         return 0.8389 - np.mean(precisions)\n",
    "#     elif np.mean(reductions) < 0.51:\n",
    "#         return 0.8640 - np.mean(precisions)\n",
    "#     else:\n",
    "#         return 1.1 - np.mean(precisions)\n",
    "\n",
    "# space = {\"idReduction\": hp.uniform(\"idReduction\", 0, 0.3),\n",
    "#          \"localityReduction\": hp.uniform(\"localityReduction\", 0, 0.1),\n",
    "#          \"n_localVarsClusters\": hp.randint(\"n_localVarsClusters\", 1, 11),\n",
    "#          \"perOperator\": hp.randint(\"perOperator\", 1, 16),\n",
    "#          \"perOpcode\": hp.randint(\"perOpcode\", 1, 41),\n",
    "#          \"perReturn\": hp.randint(\"perReturn\", 1, 31),\n",
    "#          \"perTryCatch\": hp.randint(\"perTryCatch\", 1, 31),\n",
    "#          \"perIDCluster\":hp.randint(\"perIDCluster\", 1, 3),\n",
    "#          \"perLocCluster\":hp.randint(\"perLocCluster\", 1, 11),\n",
    "#          \"perVarsCluster\": hp.randint(\"perVarsCluster\", 1, 31),\n",
    "#          \"samplingStop\": hp.uniform(\"samplingStop\", 0, 1),\n",
    "#          \"tol\": hp.uniform(\"tol\", 0.00001, 0.01),\n",
    "#          \"c\": hp.uniform(\"c\", 0.05, 1)}\n",
    "\n",
    "# best = fmin(objective, space, algo=tpe.suggest, max_evals=30)\n",
    "\n",
    "# with open(\"best.pkl\", 'wb') as outp:  # Overwrites any existing file.\n",
    "#     pickle.dump(best, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# print(best)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "584d8b4d",
   "metadata": {},
   "source": [
    "Giving the printed output of fmin to the tryout function allows you to run CPMT with those parameters.\n",
    "\n",
    "Below is the cell with 3-Projects training CPMT experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tryOut(args):\n",
    "#     parameters = [args[\"idReduction\"], args[\"localityReduction\"], args[\"n_localVarsClusters\"], args[\"perOperator\"], args[\"perOpcode\"], args[\"perReturn\"],\n",
    "#                   args[\"perTryCatch\"], args[\"perIDCluster\"], args[\"perLocCluster\"], args[\"perVarsCluster\"], args[\"samplingStop\"]]\n",
    "#     classAlgo = LinearSVC(dual=False, tol=args[\"tol\"], C=args[\"c\"])\n",
    "#     precisions = []\n",
    "#     reductions = []\n",
    "#     timings = []\n",
    "#     for pickedSeed in seeds:\n",
    "#         seed(pickedSeed)\n",
    "#         print(\"---------Starting seed: \" + str(pickedSeed) + \"---------\")\n",
    "#\n",
    "#         # Every project gets its own training.\n",
    "#         for i in range(len(projects)):\n",
    "#             # predictProject is not used for training.\n",
    "#             predictProject = projects.pop(i)\n",
    "#             classifier = trainAlgorithm(sample(projects, 3), classAlgo, parameters)\n",
    "#             start_time = time.time()\n",
    "#             results, reduction = own_predict(classifier, predictProject, parameters)\n",
    "#             timings.append(time.time() - start_time)\n",
    "#             precisions.append(precisionCalc(predictProject, results))\n",
    "#             reductions.append(reduction)\n",
    "#             projects.insert(i, predictProject)\n",
    "#\n",
    "#     print(\"RESULTS:\")\n",
    "#     for i in range(len(projects)):\n",
    "#         print(\"Average reduction for project: \" + projects[i] + \":\")\n",
    "#         print(np.mean(reductions[i:len(reductions):len(seeds)]))\n",
    "#         print(\"Average precision for project: \" + projects[i] + \":\")\n",
    "#         print(np.mean(precisions[i:len(reductions):len(seeds)]))\n",
    "#\n",
    "#     performance = [np.mean(reductions), np.mean(precisions), np.std(precisions), max(precisions), min(precisions)]\n",
    "#     print(\"timings: \" + str(timings))    \n",
    "#     print(\"performance: \" + str(performance))\n",
    "\n",
    "# tryOut({'c': 0.2063328083683676, 'idReduction': 0.01574087592571406, 'localityReduction': 0.019430303075051083, 'n_localVarsClusters': 4, 'perIDCluster': 1, 'perLocCluster': 3, 'perOpcode': 21, 'perOperator': 2, 'perReturn': 27, 'perTryCatch': 14, 'perVarsCluster': 25, 'samplingStop': 0.12912822558556847, 'tol': 0.003700890655623175})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "795371bd",
   "metadata": {},
   "source": [
    "Below is the cell with original CPMT experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12412f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryOut(args):\n",
    "    parameters = [args[\"idReduction\"], args[\"localityReduction\"], args[\"n_localVarsClusters\"], args[\"perOperator\"], args[\"perOpcode\"], args[\"perReturn\"],\n",
    "                  args[\"perTryCatch\"], args[\"perIDCluster\"], args[\"perLocCluster\"], args[\"perVarsCluster\"], args[\"samplingStop\"]]\n",
    "    classAlgo = LinearSVC(dual=False, tol=args[\"tol\"], C=args[\"c\"])\n",
    "    precisions = []\n",
    "    reductions = []\n",
    "    timings = []\n",
    "    for pickedSeed in seeds:\n",
    "        seed(pickedSeed)\n",
    "        print(\"---------Starting seed: \" + str(pickedSeed) + \"---------\")\n",
    "\n",
    "        # Every project gets its own training.\n",
    "        for i in range(len(projects)):\n",
    "            # predictProject is not used for training.\n",
    "            predictProject = projects.pop(i)\n",
    "            classifier = trainAlgorithm(projects, classAlgo, parameters)\n",
    "            start_time = time.time()\n",
    "            results, reduction = own_predict(classifier, predictProject, parameters)\n",
    "            timings.append(time.time() - start_time)\n",
    "            precisions.append(precisionCalc(predictProject, results))\n",
    "            reductions.append(reduction)\n",
    "            projects.insert(i, predictProject)\n",
    "\n",
    "    print(\"RESULTS:\")\n",
    "    for i in range(len(projects)):\n",
    "        print(\"Average reduction for project: \" + projects[i] + \":\")\n",
    "        print(np.mean(reductions[i:len(reductions):len(seeds)]))\n",
    "        print(\"Average precision for project: \" + projects[i] + \":\")\n",
    "        print(np.mean(precisions[i:len(reductions):len(seeds)]))\n",
    "\n",
    "    performance = [np.mean(reductions), np.mean(precisions), np.std(precisions), max(precisions), min(precisions)]\n",
    "    print(\"timings: \" + str(timings))\n",
    "    print(\"performance: \" + str(performance))\n",
    "\n",
    "tryOut({'c': 0.2063328083683676, 'idReduction': 0.01574087592571406, 'localityReduction': 0.019430303075051083, 'n_localVarsClusters': 4, 'perIDCluster': 1, 'perLocCluster': 3, 'perOpcode': 21, 'perOperator': 2, 'perReturn': 27, 'perTryCatch': 14, 'perVarsCluster': 25, 'samplingStop': 0.12912822558556847, 'tol': 0.003700890655623175})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c295b91a4a4f8e66f37da6a2fbf5c84e6919990d10548059361442497be2c972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
