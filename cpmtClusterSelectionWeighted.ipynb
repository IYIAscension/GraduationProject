{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e95b75f",
   "metadata": {},
   "source": [
    "This is the same implementation of CPMT as cpmtClusterSelection, except now the influence a mutant has on the pk-score is dependent on its similarity to the mutant the pk-score is for. (See pkScoreTransformer())\n",
    "\n",
    "This implementation was not discussed in Adam's thesis as it is way too time complex (Commons-text's prediction takes 16k+ seconds), because it requires a comparison between every mutant and every tested mutant for every corresponding feature.\n",
    "\n",
    "The similarity is currently calculated based the distance between mutants in all dimensions(1 dimension = 1 feature). Creating a dimensionality-reduced feature set would allow you to calculate the distance with a simple subtraction. That should create some speedup.\n",
    "\n",
    "Still, it would likely require some new innovation before being viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "from os import path\n",
    "import os\n",
    "from random import sample, seed\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import kmeans1d\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c41181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: tutorialspoint.com/file-searching-using-python\n",
    "def find_files(filename, search_path):\n",
    "   result = []\n",
    "\n",
    "    # Walking top-down from the root\n",
    "   for root, _, files in os.walk(search_path):\n",
    "      if filename in files:\n",
    "         result.append(os.path.join(root, filename))\n",
    "   return result\n",
    "\n",
    "\n",
    "# Gets characteristics and result status data from project file\n",
    "# created by pitest clustering plugin.\n",
    "def getProjectDfs(project: str) -> tuple[pd.DataFrame, pd.DataFrame] | int:\n",
    "    csv_path = \"projects/\" + project\n",
    "    charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "\n",
    "    if charPath:\n",
    "        charPath = charPath[0]\n",
    "        data = pd.read_csv(charPath,\n",
    "                            names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                    \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                    \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                            skiprows=1)\n",
    "        killedPath = find_files(\"killed.csv\", csv_path)\n",
    "\n",
    "        if killedPath:\n",
    "            killedPath = killedPath[0]\n",
    "            results = pd.read_csv(killedPath,\n",
    "                                    names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                    skiprows=1)\n",
    "            results = results.drop(columns=[\"numTests\"])\n",
    "        else:\n",
    "            print(\"Could not find killed.csv for project: \" + project)\n",
    "            return -1\n",
    "    else:\n",
    "        print(\"Could not find characteristics.csv for project: \" + project)\n",
    "        return -1\n",
    "    \n",
    "    return data, results\n",
    "\n",
    "\n",
    "# Uses label encoding and clustering to change numerical data to categorical.\n",
    "# Also merges data with results.\n",
    "def dfToCategorical(data: pd.DataFrame, results: pd.DataFrame, parameters: list) -> pd.DataFrame:\n",
    "    idReduction, localityReduction, n_localVarsClusters = parameters\n",
    "    # define ordinal encoding\n",
    "    encoder = LabelEncoder()\n",
    "    newData = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"localVarsCount\",\n",
    "                    \"isInTryCatch\", \"className\", \"methodName\", \"lineNumber\"]]\n",
    "    for col in [\"mutOperator\", \"returnType\", \"className\", \"methodName\", \"id\"]:\n",
    "        newData[col] = encoder.fit_transform(newData[col])\n",
    "\n",
    "    idClustering = kmeans1d.cluster(np.asarray(newData[[\"id\"]], dtype=\"int64\"), int(math.ceil(len(data) * idReduction)))[0]\n",
    "\n",
    "    # Categorical locality variable creation.\n",
    "    newData[\"className\"] = newData[\"className\"].apply(lambda x: x*100000)\n",
    "    newData[\"methodName\"] = newData[\"methodName\"].apply(lambda x: x*1000)\n",
    "    localityClustering = kmeans1d.cluster(np.asarray(newData[[\"className\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"methodName\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"lineNumber\"]], dtype=\"int64\"), int(math.ceil(len(data) * localityReduction)))[0]\n",
    "\n",
    "    varsClustering = kmeans1d.cluster(np.asarray(newData[[\"localVarsCount\"]], dtype=\"int64\"), n_localVarsClusters)[0]\n",
    "\n",
    "    training = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"isInTryCatch\"]]\n",
    "    training[\"idCluster_id\"] = idClustering\n",
    "    training[\"localityCluster_id\"] = localityClustering\n",
    "    training[\"varsCluster_id\"] = varsClustering\n",
    "    training = training.merge(results, how=\"inner\", on=\"id\")\n",
    "\n",
    "    return training\n",
    "\n",
    "\n",
    "# Source: https://stackoverflow.com/a/29651514\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Transforms the given dataframe for better clustering as described in Adam's thesis.\n",
    "# Single characteristic clustering is already done by dfToCategorical.\n",
    "def preprocessing(data: pd.DataFrame, parameters: list) -> pd.DataFrame:\n",
    "    idW, mutOpW, opcodeW, retTypeW, locVarsCountW, tryCatchW, localityW = parameters\n",
    "    normalizedData = normalize(data[[\"opcode\", \"varsCluster_id\", \"isInTryCatch\", \"localityCluster_id\", \"idCluster_id\"]])\n",
    "    weightedData = normalizedData.mul([opcodeW, locVarsCountW, tryCatchW, localityW, idW])\n",
    "    newData = weightedData.join(pd.get_dummies(data[\"mutOperator\"]) * mutOpW)\n",
    "    newData = newData.join(pd.get_dummies(data[\"returnType\"]) * retTypeW)\n",
    "    return newData\n",
    "\n",
    "\n",
    "# Clusters mutants and selects the mutants closest to the center of each cluster.\n",
    "# May want to change the selection method, considering center selection performed badly in CMT.\n",
    "def sampleSelector(data: pd.DataFrame, parameters: list, encoder: LabelEncoder, reduction: float) -> list:\n",
    "    newData = preprocessing(data, parameters)\n",
    "    clustering = KMeans(n_clusters=int(math.ceil(len(data) * reduction)), n_init=1)\n",
    "    clusters = clustering.fit_transform(newData)\n",
    "    clusterCenterIndices = np.argmin(clusters, axis=0)\n",
    "    mutants = data.loc[clusterCenterIndices][\"id\"]\n",
    "    return mutants, clusters\n",
    "\n",
    "\n",
    "# Calculates pk-scores, but the influence each mutant of the tested mutants has on the pk-score of a mutant\n",
    "# is weighted by its co-similarity to that mutant.\n",
    "def  pkScoreTransformer(fullDF: pd.DataFrame, sampleDF: pd.DataFrame, default: float, distances: np.ndarray) -> pd.DataFrame:\n",
    "    for charName in list(fullDF)[1:-1]:\n",
    "        for charVal in list(dict.fromkeys(fullDF[charName].tolist())):\n",
    "            try:\n",
    "                indices = fullDF.index[fullDF[\"id\"].isin(sampleDF[sampleDF[charName] == charVal][\"id\"])].tolist()\n",
    "                for mutant in fullDF.index[fullDF[\"id\"].isin(fullDF[fullDF[charName] == charVal][\"id\"])].tolist():\n",
    "                    killed = 0\n",
    "                    total = 0\n",
    "\n",
    "                    for mutantCompare in indices:\n",
    "                        # Distances[mutant] is an array where each value is the distance from a cluster centroid.\n",
    "                        # There should be no difference between using the distances relative to cluster centroid\n",
    "                        # and absolute distances (features).\n",
    "                        mutantDistance = distances[mutant]\n",
    "                        mutantCompareDistance = distances[mutantCompare]\n",
    "                        cos_sim = dot(mutantDistance, mutantCompareDistance)/(norm(mutantDistance)*norm(mutantCompareDistance))\n",
    "                        if fullDF.loc[mutantCompare, \"killed\"] == 1:\n",
    "                            killed += cos_sim\n",
    "                        total += cos_sim\n",
    "\n",
    "                    fullDF.loc[fullDF[\"id\"] == fullDF.loc[mutant, \"id\"], charName] = killed / total\n",
    "            except ZeroDivisionError:\n",
    "                fullDF.loc[fullDF[charName] == charVal, charName] = default\n",
    "\n",
    "    return fullDF\n",
    "\n",
    "\n",
    "# Takes a list of projects, transforms their characteristic data based on parameters\n",
    "# into a single CPMT-ready dataframe. Trains the given classifier on the dataframe.\n",
    "def trainAlgorithm(projects: list, classifier: any, parameters: list=[0.05, 0.05, 5, 0.01], reduction: float=0.1, prepParams: list=[13998, 8, 598, 4240, 9505, 15477, 14723]) -> any:\n",
    "    samplingStop = parameters[-1]\n",
    "    classificationTraining = []\n",
    "    for project in projects:\n",
    "        print(\"Starting training project: \" + project)\n",
    "        dataframes = getProjectDfs(project)\n",
    "        if dataframes == -1:\n",
    "            continue\n",
    "        data, results = dataframes\n",
    "\n",
    "        training = dfToCategorical(data, results, parameters[:3])\n",
    "        print(\"Starting creation of classification training set for project: \" + project)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # For every project, a CPMT-ready dataframe is inserted\n",
    "        # multiple times to produce subsampling. samplingStop controls how many.\n",
    "        allMutants = []\n",
    "        mutantsSampleDfs = []\n",
    "        i = 0\n",
    "        encoder = LabelEncoder()\n",
    "        while True:\n",
    "            print(\"iteration = \", i)\n",
    "            i += 1\n",
    "            print(samplingStop * len(training[\"id\"].tolist()), len(allMutants))\n",
    "            mutantsSample, distances = sampleSelector(training, prepParams, encoder, reduction)\n",
    "            allMutants.extend(mutantsSample)\n",
    "            trainingSample = training[training[\"id\"].isin(mutantsSample)]\n",
    "\n",
    "            # Default pk-score for if a possible characteristics value has no mutants in selection.\n",
    "            # Might want to change this by multiplying each killed mutant by the number of mutants\n",
    "            # in the cluster they represent and dividing by all mutants.\n",
    "            defaultPK = len(trainingSample[trainingSample[\"killed\"] == 1]) / len(trainingSample)\n",
    "            trainingInsert = training.copy()\n",
    "            trainingInsert = pkScoreTransformer(trainingInsert, trainingSample, defaultPK, distances)\n",
    "\n",
    "            mutantsSampleDfs.append(trainingInsert)\n",
    "\n",
    "            # Basing it on \"allMutants\" means lower reductions -> more subsampling.\n",
    "            # Might want to experiment with different subsampling if classifier gets more complex. \n",
    "            if samplingStop * len(training[\"id\"].tolist()) < len(allMutants):\n",
    "                break\n",
    "\n",
    "        # All dataframes of 1 project get combined.\n",
    "        classificationTraining.append(pd.concat(mutantsSampleDfs).drop(columns=[\"id\"]))\n",
    "\n",
    "    print(\"Done with getting all classification training data.\")\n",
    "    print(time.time() - start_time)\n",
    "\n",
    "    # Dataframes of all projects get combined.\n",
    "    classificationTrainingDf = pd.concat(classificationTraining)\n",
    "    X_train = classificationTrainingDf.drop(columns=[\"killed\"]).values.tolist()\n",
    "    y_train = classificationTrainingDf[\"killed\"].tolist()\n",
    "\n",
    "    print(\"Starting training of classifier.\")\n",
    "    start_time = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(time.time() - start_time)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Predicts the result of every mutant in the given project using CPMT.\n",
    "def own_predict(classifier: any, project: str, parameters: list=[0.05, 0.05, 5, 0.01], reduction: float=0.1, prepParams: list=[13998, 8, 598, 4240, 9505, 15477, 14723], useSampled: bool=True) -> tuple[np.ndarray, float]:\n",
    "    samplingStop = parameters[-1]\n",
    "    print(\"Starting prediction project: \" + project)\n",
    "    start_time = time.time()\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "\n",
    "    # Transformation from raw characteristic data to CPMT-ready data.\n",
    "    fullDF = dfToCategorical(data, results, parameters[:3])\n",
    "    sampleMutantsIDs, distances = sampleSelector(fullDF, prepParams, LabelEncoder(), reduction)\n",
    "    sampleDF = fullDF[fullDF[\"id\"].isin(sampleMutantsIDs)]\n",
    "    reduction = len(sampleMutantsIDs)/len(fullDF)\n",
    "    print(\"reduction = \" + str(reduction))\n",
    "    # Default pk-score for if a possible characteristics value has no mutants in selection.\n",
    "    defaultPK = len(sampleDF[sampleDF[\"killed\"] == 1]) / len(sampleDF)\n",
    "\n",
    "    trainingInsert = pkScoreTransformer(fullDF, sampleDF, defaultPK, distances)\n",
    "\n",
    "    # Saves the results of the \"executed\" mutants to overwrite the predictions.\n",
    "    if useSampled:\n",
    "        killedIndices = fullDF.index[(fullDF[\"killed\"] == 1) & (fullDF[\"id\"].isin(sampleMutantsIDs))].tolist()\n",
    "        survivedIndices = fullDF.index[(fullDF[\"killed\"] == 0) & (fullDF[\"id\"].isin(sampleMutantsIDs))].tolist()\n",
    "    fullDF = fullDF.drop(columns=[\"id\", \"killed\"])\n",
    "\n",
    "    predictResults = classifier.predict(fullDF)\n",
    "\n",
    "    # For the executed mutants, CPMT would not need to predict their result.\n",
    "    if useSampled:\n",
    "        predictResults[killedIndices] = 1\n",
    "        predictResults[survivedIndices] = 0\n",
    "    print(\"Prediction for project \" + project + \" took: \" + str(time.time() - start_time) + \" seconds.\")\n",
    "    return predictResults, reduction\n",
    "\n",
    "\n",
    "# Calculates the precision: percentage of correct predictions in decimals.\n",
    "def precisionCalc(project: str, predictions: np.ndarray) -> float:\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "    data[\"prediction\"] = predictions\n",
    "    newData = data[[\"id\", \"prediction\"]]\n",
    "    merged = newData.merge(results, how=\"inner\", on=\"id\")\n",
    "    merged.to_csv(\"projects/\" + project + \"/predictions.csv\", sep=\",\", index=False)\n",
    "    correctList = [1 if i == j else 0 for i, j in zip(merged[\"killed\"].tolist(), merged[\"prediction\"].tolist())]\n",
    "    precision = len([1 for val in correctList if val == 1 ])/len(correctList)\n",
    "    print(\"Precision = \" + str(precision))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544c776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "    66304, 16389, 14706, 91254, 49890, 86054, 55284, 77324, 36147, 13506, 73920, 80157, 43981, 75358, 33399, 56134,\n",
    "    13388, 81617, 90957, 52113, 20428, 26482, 56340, 31018, 32067, 13067, 8339, 49008, 125894, 68282, ]\n",
    "trainingProjects = [\"google-auto-service\", \"scribejava-core\",\n",
    "                     \"commons-cli\", \"google-auto-value\", \"gson\", \"commons-io\", \"commons-codec\"]\n",
    "projects = [ \"google-auto-factory\", \"google-auto-common\", \"commons-csv\", \"commons-text\"]\n",
    "parameters = [0.05, 5, 3, 3, 3, 3, 2, 2, 2]\n",
    "# classifier = SGDClassifier()\n",
    "reduction = [0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23077e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12412f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training project: google-auto-service\n",
      "Starting creation of classification training set for project: google-auto-service\n",
      "iteration =  0\n",
      "4.75 0\n",
      "Starting training project: scribejava-core\n",
      "Starting creation of classification training set for project: scribejava-core\n",
      "iteration =  0\n",
      "57.46 0\n",
      "Starting training project: commons-cli\n",
      "Starting creation of classification training set for project: commons-cli\n",
      "iteration =  0\n",
      "71.9 0\n",
      "Done with getting all classification training data.\n",
      "330.33147859573364\n",
      "Starting training of classifier.\n",
      "0.025006771087646484\n",
      "Starting prediction project: google-auto-factory\n",
      "reduction = 0.10013717421124829\n",
      "Prediction for project google-auto-factory took: 206.50364136695862 seconds.\n",
      "Precision = 0.8335048010973937\n",
      "Starting prediction project: google-auto-common\n",
      "reduction = 0.10001916075876605\n",
      "Prediction for project google-auto-common took: 155.86904740333557 seconds.\n",
      "Precision = 0.7834834259436674\n",
      "Starting prediction project: commons-csv\n",
      "reduction = 0.10005792064871126\n",
      "Prediction for project commons-csv took: 323.69273042678833 seconds.\n",
      "Precision = 0.869244135534318\n",
      "Starting prediction project: commons-text\n",
      "reduction = 0.1\n",
      "Prediction for project commons-text took: 16152.803048849106 seconds.\n",
      "Precision = 0.8106619921633327\n",
      "[0.1000535639046814, 0.824223588684678]\n"
     ]
    }
   ],
   "source": [
    "def tryOut(args):\n",
    "    parameters = [args[\"localityReduction\"], args[\"n_localVarsClusters\"], args[\"perOperator\"], args[\"perOpcode\"], args[\"perReturn\"],\n",
    "                  args[\"perTryCatch\"], args[\"perLocCluster\"], args[\"perVarsCluster\"], args[\"samplingStop\"]]\n",
    "    classAlgo = LinearSVC(dual=False, tol=args[\"tol\"], C=args[\"c\"])\n",
    "    try:\n",
    "        classifier = trainAlgorithm(trainingProjects[:3], classAlgo)\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "    precisions = []\n",
    "    reductions = []\n",
    "    for project in projects:\n",
    "        results, reduction = own_predict(classifier, project)\n",
    "        precisions.append(precisionCalc(project, results))\n",
    "        reductions.append(reduction)\n",
    "    performance = [np.mean(reductions), np.mean(precisions)]\n",
    "    print(performance)\n",
    "\n",
    "tryOut({'c': 0.498087050393805, 'localityReduction': 0.01582020546979156, 'n_localVarsClusters': 2, 'perLocCluster': 1, 'perOpcode': 27, 'perOperator': 10, 'perReturn': 17, 'perTryCatch': 4, 'perVarsCluster': 22, 'samplingStop': 0.1537291955327681, 'tol': 0.005117505124731138})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c295b91a4a4f8e66f37da6a2fbf5c84e6919990d10548059361442497be2c972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
