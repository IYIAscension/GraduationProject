{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0211f43f",
   "metadata": {},
   "source": [
    "This file was made to calculate the precision of Contextual Predictive Mutation Testing, but mutants are selected from clusters instead of from characteristic values.\n",
    "\n",
    "More in-depth discussion is in Adam Abdalla's thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import pickle\n",
    "from os import path\n",
    "import os\n",
    "from random import sample, seed\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import kmeans1d\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: tutorialspoint.com/file-searching-using-python\n",
    "def find_files(filename, search_path):\n",
    "   result = []\n",
    "\n",
    "    # Walking top-down from the root\n",
    "   for root, _, files in os.walk(search_path):\n",
    "      if filename in files:\n",
    "         result.append(os.path.join(root, filename))\n",
    "   return result\n",
    "\n",
    "\n",
    "# Gets characteristics and result status data from project file\n",
    "# created by pitest clustering plugin.\n",
    "def getProjectDfs(project: str) -> tuple[pd.DataFrame, pd.DataFrame] | int:\n",
    "    csv_path = \"projects/\" + project\n",
    "    charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "\n",
    "    if charPath:\n",
    "        charPath = charPath[0]\n",
    "        data = pd.read_csv(charPath,\n",
    "                            names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                    \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                    \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                            skiprows=1)\n",
    "        killedPath = find_files(\"killed.csv\", csv_path)\n",
    "\n",
    "        if killedPath:\n",
    "            killedPath = killedPath[0]\n",
    "            results = pd.read_csv(killedPath,\n",
    "                                    names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                    skiprows=1)\n",
    "            results = results.drop(columns=[\"numTests\"])\n",
    "        else:\n",
    "            print(\"Could not find killed.csv for project: \" + project)\n",
    "            return -1\n",
    "    else:\n",
    "        print(\"Could not find characteristics.csv for project: \" + project)\n",
    "        return -1\n",
    "\n",
    "    return data, results\n",
    "\n",
    "\n",
    "# Uses label encoding and clustering to change numerical data to categorical.\n",
    "# Also merges data with results.\n",
    "def dfToCategorical(data: pd.DataFrame, results: pd.DataFrame, parameters: list) -> pd.DataFrame:\n",
    "    idReduction, localityReduction, n_localVarsClusters = parameters\n",
    "    # define ordinal encoding\n",
    "    encoder = LabelEncoder()\n",
    "    newData = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"localVarsCount\",\n",
    "                    \"isInTryCatch\", \"className\", \"methodName\", \"lineNumber\"]]\n",
    "    for col in [\"mutOperator\", \"returnType\", \"className\", \"methodName\", \"id\"]:\n",
    "        newData[col] = encoder.fit_transform(newData[col])\n",
    "\n",
    "    idClustering = kmeans1d.cluster(np.asarray(newData[[\"id\"]], dtype=\"int64\"), int(math.ceil(len(data) * idReduction)))[0]\n",
    "\n",
    "    # Categorical locality variable creation.\n",
    "    newData[\"className\"] = newData[\"className\"].apply(lambda x: x*100000)\n",
    "    newData[\"methodName\"] = newData[\"methodName\"].apply(lambda x: x*1000)\n",
    "    localityClustering = kmeans1d.cluster(np.asarray(newData[[\"className\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"methodName\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"lineNumber\"]], dtype=\"int64\"), int(math.ceil(len(data) * localityReduction)))[0]\n",
    "\n",
    "    varsClustering = kmeans1d.cluster(np.asarray(newData[[\"localVarsCount\"]], dtype=\"int64\"), n_localVarsClusters)[0]\n",
    "\n",
    "    training = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"isInTryCatch\"]]\n",
    "    training[\"idCluster_id\"] = idClustering\n",
    "    training[\"localityCluster_id\"] = localityClustering\n",
    "    training[\"varsCluster_id\"] = varsClustering\n",
    "    training = training.merge(results, how=\"inner\", on=\"id\")\n",
    "\n",
    "    return training\n",
    "\n",
    "\n",
    "# Source: https://stackoverflow.com/a/29651514\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Transforms the given dataframe for better clustering as described in Adam's thesis.\n",
    "# Single characteristic clustering is already done by dfToCategorical.\n",
    "def preprocessing(data: pd.DataFrame, parameters: list) -> pd.DataFrame:\n",
    "    idW, mutOpW, opcodeW, retTypeW, locVarsCountW, tryCatchW, localityW = parameters\n",
    "    normalizedData = normalize(data[[\"opcode\", \"varsCluster_id\", \"isInTryCatch\", \"localityCluster_id\", \"idCluster_id\"]])\n",
    "    weightedData = normalizedData.mul([opcodeW, locVarsCountW, tryCatchW, localityW, idW])\n",
    "    newData = weightedData.join(pd.get_dummies(data[\"mutOperator\"]) * mutOpW)\n",
    "    newData = newData.join(pd.get_dummies(data[\"returnType\"]) * retTypeW)\n",
    "    return newData\n",
    "\n",
    "\n",
    "# Clusters mutants and selects the mutants closest to the center of each cluster.\n",
    "# May want to change the selection method, considering center selection performed badly in CMT.\n",
    "def sampleSelector(data: pd.DataFrame, parameters: list, encoder: LabelEncoder, reduction: float) -> list:\n",
    "    newData = preprocessing(data, parameters)\n",
    "    clustering = KMeans(n_clusters=int(math.ceil(len(data) * reduction)), n_init=1)\n",
    "    clusters = clustering.fit_transform(newData)\n",
    "    clusterCenterIndices = np.argmin(clusters, axis=0)\n",
    "    mutants = data.loc[clusterCenterIndices][\"id\"]\n",
    "    return mutants\n",
    "\n",
    "\n",
    "# For every possible value of every characteristic, takes all mutants with that value in the sampleDF.\n",
    "# Calculates the percentage of those mutants which are killed, aka pk-score.\n",
    "# Overwrites the value in the fullDF with the pk-score. Returns new fullDF.\n",
    "def  pkScoreTransformer(fullDF: pd.DataFrame, sampleDF: pd.DataFrame, default: float) -> pd.DataFrame:\n",
    "    for charName in list(fullDF[1:-1]):\n",
    "        for charVal in list(dict.fromkeys(fullDF[charName].tolist())):\n",
    "            try:\n",
    "                percentageKilled = len(sampleDF[(sampleDF[charName] == charVal) & (sampleDF[\"killed\"] == 1)]) / len(sampleDF[sampleDF[charName] == charVal])\n",
    "            except ZeroDivisionError:\n",
    "                percentageKilled = default\n",
    "\n",
    "            fullDF.loc[fullDF[charName] == charVal, charName] = percentageKilled\n",
    "\n",
    "    return fullDF\n",
    "\n",
    "\n",
    "# Takes a list of projects, transforms their characteristic data based on parameters\n",
    "# into a single CPMT-ready dataframe. Trains the given classifier on the dataframe.\n",
    "def trainAlgorithm(projects: list, classifier: any, parameters: list=[0.05, 0.05, 5, 0.01], reduction: float=0.1, prepParams: list=[13998, 8, 598, 4240, 9505, 15477, 14723]) -> any:\n",
    "    samplingStop = parameters[-1]\n",
    "    classificationTraining = []\n",
    "    for project in projects:\n",
    "        print(\"Starting training project: \" + project)\n",
    "        dataframes = getProjectDfs(project)\n",
    "        if dataframes == -1:\n",
    "            continue\n",
    "        data, results = dataframes\n",
    "\n",
    "        training = dfToCategorical(data, results, parameters[:3])\n",
    "        print(\"Starting creation of classification training set for project: \" + project)\n",
    "        start_time = time.time()\n",
    "\n",
    "        # For every project, a CPMT-ready dataframe is inserted\n",
    "        # multiple times to produce subsampling. samplingStop controls how many.\n",
    "        allMutants = []\n",
    "        mutantsSampleDfs = []\n",
    "        i = 0\n",
    "        encoder = LabelEncoder()\n",
    "        while True:\n",
    "            print(\"iteration = \", i)\n",
    "            i += 1\n",
    "            print(samplingStop * len(training[\"id\"].tolist()), len(allMutants))\n",
    "            mutantsSample = sampleSelector(training, prepParams, encoder, reduction)\n",
    "            allMutants.extend(mutantsSample)\n",
    "            trainingSample = training[training[\"id\"].isin(mutantsSample)]\n",
    "\n",
    "            # Default pk-score for if a possible characteristics value has no mutants in selection.\n",
    "            # Might want to change this by multiplying each killed mutant by the number of mutants\n",
    "            # in the cluster they represent and dividing by all mutants.\n",
    "            defaultPK = len(trainingSample[trainingSample[\"killed\"] == 1]) / len(trainingSample)\n",
    "            trainingInsert = training.copy()\n",
    "            trainingInsert = pkScoreTransformer(trainingInsert, trainingSample, defaultPK)\n",
    "\n",
    "            mutantsSampleDfs.append(trainingInsert)\n",
    "\n",
    "            # Basing it on \"allMutants\" means lower reductions -> more subsampling.\n",
    "            # Might want to experiment with different subsampling if classifier gets more complex.\n",
    "            if samplingStop * len(training[\"id\"].tolist()) < len(allMutants):\n",
    "                break\n",
    "\n",
    "        # All dataframes of 1 project get combined.\n",
    "        classificationTraining.append(pd.concat(mutantsSampleDfs).drop(columns=[\"id\"]))\n",
    "\n",
    "    print(\"Done with getting all classification training data.\")\n",
    "    print(time.time() - start_time)\n",
    "\n",
    "    # Dataframes of all projects get combined.\n",
    "    classificationTrainingDf = pd.concat(classificationTraining)\n",
    "    X_train = classificationTrainingDf.drop(columns=[\"killed\"]).values.tolist()\n",
    "    y_train = classificationTrainingDf[\"killed\"].tolist()\n",
    "\n",
    "    print(\"Starting training of classifier.\")\n",
    "    start_time = time.time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(time.time() - start_time)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Predicts the result of every mutant in the given project using CPMT.\n",
    "def own_predict(classifier: any, project: str, parameters: list=[0.05, 0.05, 5, 0.01], reduction: float=0.1, prepParams: list=[13998, 8, 598, 4240, 9505, 15477, 14723], useSampled: bool=True) -> tuple[np.ndarray, float]:\n",
    "    samplingStop = parameters[-1]\n",
    "    print(\"Starting prediction project: \" + project)\n",
    "    start_time = time.time()\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "\n",
    "    # Transformation from raw characteristic data to CPMT-ready data.\n",
    "    fullDF = dfToCategorical(data, results, parameters[:3])\n",
    "    sampleMutantsIDs = sampleSelector(fullDF, prepParams, LabelEncoder(), reduction)\n",
    "    sampleDF = fullDF[fullDF[\"id\"].isin(sampleMutantsIDs)]\n",
    "    reduction = len(sampleMutantsIDs)/len(fullDF)\n",
    "    print(\"reduction = \" + str(reduction))\n",
    "    # Default pk-score for if a possible characteristics value has no mutants in selection.\n",
    "    defaultPK = len(sampleDF[sampleDF[\"killed\"] == 1]) / len(sampleDF)\n",
    "\n",
    "    trainingInsert = pkScoreTransformer(fullDF, sampleDF, defaultPK)\n",
    "\n",
    "    # Saves the results of the \"executed\" mutants to overwrite the predictions.\n",
    "    if useSampled:\n",
    "        killedIndices = fullDF.index[(fullDF[\"killed\"] == 1) & (fullDF[\"id\"].isin(sampleMutantsIDs))].tolist()\n",
    "        survivedIndices = fullDF.index[(fullDF[\"killed\"] == 0) & (fullDF[\"id\"].isin(sampleMutantsIDs))].tolist()\n",
    "    fullDF = fullDF.drop(columns=[\"id\", \"killed\"])\n",
    "\n",
    "    predictResults = classifier.predict(fullDF)\n",
    "\n",
    "    # For the executed mutants, CPMT would not need to predict their result.\n",
    "    if useSampled:\n",
    "        predictResults[killedIndices] = 1\n",
    "        predictResults[survivedIndices] = 0\n",
    "    print(\"Prediction for project \" + project + \" took: \" + str(time.time() - start_time) + \" seconds.\")\n",
    "    return predictResults, reduction\n",
    "\n",
    "\n",
    "# Calculates the precision: percentage of correct predictions in decimals.\n",
    "def precisionCalc(project: str, predictions: np.ndarray) -> float:\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "    data[\"prediction\"] = predictions\n",
    "    newData = data[[\"id\", \"prediction\"]]\n",
    "    merged = newData.merge(results, how=\"inner\", on=\"id\")\n",
    "    merged.to_csv(\"projects/\" + project + \"/predictions.csv\", sep=\",\", index=False)\n",
    "    correctList = [1 if i == j else 0 for i, j in zip(merged[\"killed\"].tolist(), merged[\"prediction\"].tolist())]\n",
    "    precision = len([1 for val in correctList if val == 1 ])/len(correctList)\n",
    "    print(\"Precision = \" + str(precision))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166ad15",
   "metadata": {},
   "source": [
    "Parameter cell down below. Note that these are not the only parameters you might want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "    66304, 16389, 14706, 91254, 49890, 86054, 55284, 77324, 36147, 13506, 73920, 80157, 43981, 75358, 33399, 56134,\n",
    "    13388, 81617, 90957, 52113, 20428, 26482, 56340, 31018, 32067, 13067, 8339, 49008, 125894, 68282, ]\n",
    "# trainingProjects = [\"google-auto-service\", \"scribejava-core\", \"commons-cli\",\n",
    "#                     \"google-auto-value\",] # \"gson\", \"commons-io\", \"commons-codec\"\n",
    "projects = [\"google-auto-common\", \"scribejava-core\", \"google-auto-factory\", \"commons-csv\",\n",
    "                \"commons-cli\", \"google-auto-value\", \"gson\", \"commons-io\",\"commons-text\", \"commons-codec\", ]\n",
    "parameters = [0.05, 5, 3, 3, 3, 3, 2, 2, 2]\n",
    "# classifier = SGDClassifier()\n",
    "reduction = [0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23077e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c951ed42",
   "metadata": {},
   "source": [
    "Below is the cell with CPMT experiments using cluster center selection for the mutant selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12412f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopt was not used for ClusterSelection's parameters or prepParams.\n",
    "# We instead used cmtImprovement's values for prepParams and some basic\n",
    "# values for parameters, these are default arguments of the functions.\n",
    "def tryOut(args):\n",
    "    # parameters = [args[\"localityReduction\"], args[\"n_localVarsClusters\"], args[\"perOperator\"], args[\"perOpcode\"], args[\"perReturn\"],\n",
    "    #               args[\"perTryCatch\"], args[\"perLocCluster\"], args[\"perVarsCluster\"], args[\"samplingStop\"]]\n",
    "    classAlgo = LinearSVC(dual=False, tol=args[\"tol\"], C=args[\"c\"])\n",
    "    precisions = []\n",
    "    reductions = []\n",
    "    for i in range(len(projects)):\n",
    "        predictProject = projects.pop(i)\n",
    "        try:\n",
    "            classifier = trainAlgorithm(projects, classAlgo)\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "        results, reduction = own_predict(classifier, predictProject)\n",
    "        precisions.append(precisionCalc(predictProject, results))\n",
    "        reductions.append(reduction)\n",
    "        projects.insert(i, predictProject)\n",
    "    performance = [np.mean(reductions), np.mean(precisions), np.std(precisions), max(precisions), min(precisions)]\n",
    "    print(performance)\n",
    "\n",
    "tryOut({'c': 0.498087050393805, 'localityReduction': 0.01582020546979156, 'n_localVarsClusters': 2, 'perLocCluster': 1, 'perOpcode': 27, 'perOperator': 10, 'perReturn': 17, 'perTryCatch': 4, 'perVarsCluster': 22, 'samplingStop': 0.1537291955327681, 'tol': 0.005117505124731138})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c295b91a4a4f8e66f37da6a2fbf5c84e6919990d10548059361442497be2c972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
