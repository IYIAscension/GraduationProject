{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db618928",
   "metadata": {},
   "source": [
    "This file implements Network Mutation Testing.\n",
    "\n",
    "This was designed by Adam Abdalla, but the results lacked the promise necessary to continue working on it over CPMT.\n",
    "\n",
    "If you would like to pick it back up, I recommend the starting out with the following:\n",
    "\n",
    "-No level of precision makes the current time taken worth it, so determine what part takes longest and find a way to speed it up. (Consider libraries?)\n",
    "\n",
    "-Include all characteristics for the connections.\n",
    "\n",
    "-Use hyperopt to find optimal weights for all characteristics\n",
    "\n",
    "-Remove characteristics with insignificant weights\n",
    "\n",
    "-Find a way to determine the optimal treshold. It may require a complex calculation as it could be dependent on a lot of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from os import path\n",
    "import os\n",
    "from random import sample, seed\n",
    "import csv\n",
    "import pandas as pd\n",
    "import kmeans1d\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: tutorialspoint.com/file-searching-using-python\n",
    "def find_files(filename, search_path):\n",
    "   result = []\n",
    "\n",
    "    # Walking top-down from the root\n",
    "   for root, _, files in os.walk(search_path):\n",
    "      if filename in files:\n",
    "         result.append(os.path.join(root, filename))\n",
    "   return result\n",
    "\n",
    "# Source: https://stackoverflow.com/a/29651514\n",
    "def normalize(df: pd.DataFrame, featureIndices: list) -> pd.DataFrame:\n",
    "    result = df.copy()\n",
    "\n",
    "    for feature_name in df.columns[featureIndices]:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Gets characteristics and result status data from project file\n",
    "# created by pitest clustering plugin.\n",
    "def getProjectDfs(project: str) -> tuple[pd.DataFrame, pd.DataFrame] | int:\n",
    "    csv_path = \"projects/\" + project\n",
    "    charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "\n",
    "    if charPath:\n",
    "        charPath = charPath[0]\n",
    "        data = pd.read_csv(charPath,\n",
    "                            names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                    \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                    \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                            skiprows=1)\n",
    "        killedPath = find_files(\"killed.csv\", csv_path)\n",
    "\n",
    "        if killedPath:\n",
    "            killedPath = killedPath[0]\n",
    "            results = pd.read_csv(killedPath,\n",
    "                                    names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                    skiprows=1)\n",
    "            results = results.drop(columns=[\"numTests\"])\n",
    "        else:\n",
    "            print(\"Could not find killed.csv for project: \" + project)\n",
    "            return -1\n",
    "    else:\n",
    "        print(\"Could not find characteristics.csv for project: \" + project)\n",
    "        return -1\n",
    "\n",
    "    return data, results\n",
    "\n",
    "\n",
    "# Uses label encoding and clustering to change numerical data to categorical.\n",
    "# Also merges data with results.\n",
    "def dfToCategorical(data: pd.DataFrame, results: pd.DataFrame, parameters: list) -> pd.DataFrame:\n",
    "    localityReduction, n_localVarsClusters = parameters\n",
    "    # define ordinal encoding\n",
    "    encoder = LabelEncoder()\n",
    "    newData = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"localVarsCount\",\n",
    "                    \"isInTryCatch\", \"className\", \"methodName\", \"lineNumber\"]]\n",
    "    for col in [\"mutOperator\", \"returnType\", \"className\", \"methodName\"]:\n",
    "        newData[col] = encoder.fit_transform(newData[col])\n",
    "\n",
    "    # Categorical locality variable creation.\n",
    "    newData[\"className\"] = newData[\"className\"].apply(lambda x: x*100000)\n",
    "    newData[\"methodName\"] = newData[\"methodName\"].apply(lambda x: x*1000)\n",
    "    localityClustering = kmeans1d.cluster(np.asarray(newData[[\"className\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"methodName\"]], dtype=\"int64\") +\n",
    "                                          np.asarray(newData[[\"lineNumber\"]], dtype=\"int64\"), int(math.ceil(len(data) * localityReduction)))[0]\n",
    "\n",
    "    varsClustering = kmeans1d.cluster(np.asarray(newData[[\"localVarsCount\"]], dtype=\"int64\"), n_localVarsClusters)[0]\n",
    "\n",
    "    training = newData[[\"id\", \"mutOperator\", \"opcode\", \"returnType\", \"isInTryCatch\"]]\n",
    "    training[\"localityCluster_id\"] = localityClustering\n",
    "    training[\"varsCluster_id\"] = varsClustering\n",
    "    training = training.merge(results, how=\"inner\", on=\"id\")\n",
    "    training = normalize(training, range(3, 6))\n",
    "\n",
    "    return training\n",
    "\n",
    "\n",
    "# Creates a dictionary where each mutant has a dictionary as its value.\n",
    "# In this dictionary are all the mutants connected to it as keys\n",
    "# with the corresponding total weight of the connection as the value.\n",
    "def graphCreator(fullDF: pd.DataFrame, weights: list) -> dict:\n",
    "    graph = {}\n",
    "    for i, col in enumerate([\"returnType\", \"isInTryCatch\", \"localityCluster_id\", \"varsCluster_id\"]):\n",
    "        for val in list(dict.fromkeys(fullDF[col].tolist())):\n",
    "            sharedValList = fullDF[fullDF[col] == val].index.tolist()\n",
    "            for idx in sharedValList:\n",
    "                if idx not in graph:\n",
    "                    graph[idx] = {}\n",
    "                sharedValListCopy = sharedValList.copy()\n",
    "                sharedValListCopy.remove(idx)\n",
    "                for idx2 in sharedValListCopy:\n",
    "                    if idx2 in graph[idx]:\n",
    "                        graph[idx][idx2] *= weights[i]\n",
    "                    else:\n",
    "                        graph[idx][idx2] = weights[i]\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "# Selects which mutants should be executed.\n",
    "def sampleSelector(graph: dict, fullDF: pd.DataFrame, add: float, reduction: float) -> tuple[list, float]:\n",
    "    notInGraph = list(graph.keys())\n",
    "    inSelection = []\n",
    "    currentRed = 0\n",
    "    totalWeights = {}\n",
    "\n",
    "    # The totalweight is the sum of the weight of each connection a mutant has.\n",
    "    for idx in graph:\n",
    "        totalWeight = 0\n",
    "        for idx2 in graph[idx]:\n",
    "            totalWeight += graph[idx][idx2] * 10\n",
    "        totalWeights[idx] = totalWeight\n",
    "\n",
    "    # The while loop selects the mutants with the largest totalweight.\n",
    "    # It then reduces the totalweight for each mutant connected to the tested ones\n",
    "    # for the next loop. This causes the next loop to put more priority on\n",
    "    # selecting mutants which connect mutants not yet connected.\n",
    "    # It would be too costly to do this for every mutant selection,\n",
    "    # so the \"add\" parameter decides how many mutants get selected before recalculation.\n",
    "    while currentRed < reduction:\n",
    "        sortedIdxs = sorted(list(totalWeights.items()), key=lambda x: x[1])\n",
    "        selectedList = [x[0] for x in sortedIdxs[:math.ceil(len(fullDF) * add)]]\n",
    "        added = selectedList.copy()\n",
    "\n",
    "        for selected in selectedList:\n",
    "            added.extend(list(graph[selected].keys()))\n",
    "            del totalWeights[selected]\n",
    "\n",
    "        added = list(dict.fromkeys(added))\n",
    "        notInGraph = list(set(notInGraph)-set(added))\n",
    "        inSelection.extend(selectedList)\n",
    "\n",
    "        for idx2 in added:\n",
    "            for idx in graph[idx2]:\n",
    "                if idx in totalWeights:\n",
    "                    totalWeights[idx] -= graph[idx][idx2] * 9\n",
    "\n",
    "        currentRed = len(inSelection) / len(fullDF)\n",
    "\n",
    "    return inSelection, reduction\n",
    "\n",
    "\n",
    "# Predictors store how much of the weight of the executed mutants\n",
    "# connected to each mutant was killed.\n",
    "# Predictions stores the results of the executed mutants in the predictions dict\n",
    "# since we no longer need to calculate those.\n",
    "def predictorScores(fullDF: pd.DataFrame, inSelection: list, graph: dict) -> tuple[dict, dict]:\n",
    "    predictors = {}\n",
    "    predictions = {}\n",
    "\n",
    "    for idx in inSelection:\n",
    "        killed = fullDF.loc[idx, \"killed\"]\n",
    "        predictions[idx] = killed\n",
    "\n",
    "        for idx2 in graph[idx]:\n",
    "            weight = graph[idx][idx2]\n",
    "\n",
    "            if killed:\n",
    "                killedWeight = weight\n",
    "            else:\n",
    "                killedWeight = 0\n",
    "\n",
    "            if idx2 in predictors:\n",
    "                predictors[idx2][0] += killedWeight\n",
    "                predictors[idx2][1] += weight\n",
    "            else:\n",
    "                predictors[idx2] = [killedWeight, weight]\n",
    "\n",
    "    return predictions, predictors\n",
    "\n",
    "\n",
    "# Returns the predictions for all mutants of given project.\n",
    "def own_predict(project: str, reduction: float=0.1, add: float=0.01, parameters: list=[0.05, 5], weights: list=[2, 5, 5, 4]) -> tuple[np.ndarray, float]:\n",
    "    localityReduction, n_localVarsClusters = parameters\n",
    "    print(\"Starting prediction project: \" + project)\n",
    "    start_time = time.time()\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "\n",
    "    # Transformation from raw characteristic data to NMT-ready data.\n",
    "    fullDF = dfToCategorical(data, results, parameters)\n",
    "    graph = graphCreator(fullDF, weights)\n",
    "    inSelection, reduction = sampleSelector(graph, fullDF, add, reduction)\n",
    "    print(\"Reduction = \" + str(reduction))\n",
    "    predictions, predictors = predictorScores(fullDF, inSelection, graph)\n",
    "\n",
    "    # The algorithm should predict a mutant as killed if a certain percentage of its weight\n",
    "    # is from killed mutants. I was using this to check for the optimal treshold.\n",
    "    # The treshold should probably differ for each project. Not sure how to calculate it.\n",
    "    percentageKilled = len(np.flatnonzero(np.asarray(predictions.values())))/len(predictions)\n",
    "    for i in range(100):\n",
    "        treshold = 0.90 + i * 0.001\n",
    "        print(treshold)\n",
    "        for idx in predictors:\n",
    "            if predictors[idx][0] > predictors[idx][1]*treshold:\n",
    "                predictions[idx] = 1\n",
    "            else:\n",
    "                predictions[idx] = 0\n",
    "\n",
    "        precisionCalc(project, [prediction for idx, prediction in sorted(list(predictions.items()), key = lambda x: x[0])])\n",
    "\n",
    "    return predictions, reduction\n",
    "\n",
    "# Calculates the precision: percentage of correct predictions in decimals.\n",
    "def precisionCalc(project: str, predictions: np.ndarray) -> float:\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "    data, results = dataframes\n",
    "    data[\"prediction\"] = predictions\n",
    "    newData = data[[\"id\", \"prediction\"]]\n",
    "    merged = newData.merge(results, how=\"inner\", on=\"id\")\n",
    "    merged.to_csv(\"projects/\" + project + \"/predictions.csv\", sep=\",\", index=False)\n",
    "    correctList = [1 if i == j else 0 for i, j in zip(merged[\"killed\"].tolist(), merged[\"prediction\"].tolist())]\n",
    "    precision = len([1 for val in correctList if val == 1 ])/len(correctList)\n",
    "    print(\"Precision = \" + str(precision))\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "    66304, 16389, 14706, 91254, 49890, 86054, 55284, 77324, 36147, 13506, 73920, 80157, 43981, 75358, 33399, 56134,\n",
    "    13388, 81617, 90957, 52113, 20428, 26482, 56340, 31018, 32067, 13067, 8339, 49008, 125894, 68282, ]\n",
    "trainingProjects = [\"google-auto-service\", \"scribejava-core\", \"commons-cli\",\n",
    "                    \"google-auto-value\",\"gson\", \"commons-io\", \"commons-codec\"]\n",
    "projects = [ \"google-auto-factory\", \"google-auto-common\", \"commons-csv\", \"commons-text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23077e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12412f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryOut(args=[]):\n",
    "    precisions = []\n",
    "    reductions = []\n",
    "    for project in projects:\n",
    "        start_time = time.time()\n",
    "        results, reduction = own_predict(project)\n",
    "        print(time.time() - start_time)\n",
    "        precisions.append(precisionCalc(project, [prediction for idx, prediction in sorted(list(results.items()), key = lambda x: x[0])]))\n",
    "        reductions.append(reduction)\n",
    "    performance = np.mean(precisions)\n",
    "    reduction = np.mean(reductions)\n",
    "    print(performance, reduction)\n",
    "\n",
    "tryOut()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c295b91a4a4f8e66f37da6a2fbf5c84e6919990d10548059361442497be2c972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
