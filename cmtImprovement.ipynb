{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5b44cec",
   "metadata": {},
   "source": [
    "This file was made to calculate the impact more rigorous pre-processing and center selection has on the precision of Rasjaad Basarat's Clustered Mutation Testing algorithm.\n",
    "\n",
    "More in-depth discussion is in Adam Abdalla's thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from os import path\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "import warnings\n",
    "import pickle\n",
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: tutorialspoint.com/file-searching-using-python\n",
    "def find_files(filename: str, search_path: str) -> list:\n",
    "   result = []\n",
    "\n",
    "    # Walking top-down from the root\n",
    "   for root, _, files in os.walk(search_path):\n",
    "      if filename in files:\n",
    "         result.append(path.join(root, filename))\n",
    "\n",
    "   return result\n",
    "\n",
    "\n",
    "# Gets characteristics and result status data from project file\n",
    "# created by pitest clustering plugin.\n",
    "def getProjectDfs(project: str) -> tuple[pd.DataFrame, pd.DataFrame] | int:\n",
    "    csv_path = \"projects/\" + project\n",
    "    charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "\n",
    "    if charPath:\n",
    "        charPath = charPath[0]\n",
    "        data = pd.read_csv(charPath,\n",
    "                           names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                    \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                    \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                           skiprows=1)\n",
    "        # Normally 1 would need to execute during prediction instead of checking the file for results.\n",
    "        killedPath = find_files(\"killed.csv\", csv_path)\n",
    "\n",
    "        if killedPath:\n",
    "            killedPath = killedPath[0]\n",
    "            results = pd.read_csv(killedPath,\n",
    "                                  names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                  skiprows=1)\n",
    "            results = results.drop(columns=[\"numTests\"])\n",
    "        else:\n",
    "            print(\"Could not find killed.csv for project: \" + project)\n",
    "            return -1\n",
    "    else:\n",
    "        print(\"Could not find characteristics.csv for project: \" + project)\n",
    "        return -1\n",
    "\n",
    "    return data, results\n",
    "\n",
    "\n",
    "# Source: https://stackoverflow.com/a/29651514\n",
    "def normalize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result = df.copy()\n",
    "\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Transforms the given dataframe for better clustering as described in Adam's thesis.\n",
    "def preprocessing(data: pd.DataFrame, weights: list) -> pd.DataFrame:\n",
    "    idW, mutOpW, opcodeW, retTypeW, locVarsCountW, tryCatchW, bNumberW, localityW = weights\n",
    "\n",
    "    # A mutant is more local than another if it is in the same class,\n",
    "    # regardless of the other variables, therefore 10^5 multiplication.\n",
    "    # 10^3 means the method also gets preferred over line number.\n",
    "    data[\"locality\"] = data[\"className\"]*100000 + data[\"methodName\"]*1000 + data[\"lineNumber\"]\n",
    "\n",
    "    # Data gets normalized so original ranges no longer matter.\n",
    "    # For some of these variables, this *may* be a bad thing,\n",
    "    # For example, a large project will have locality matter relatively less.\n",
    "    normalizedData = normalize(data[[\"id\", \"opcode\", \"localVarsCount\", \"isInTryCatch\", \"blockNumber\", \"locality\"]])\n",
    "\n",
    "    # Weights and one-hot-encoded characteristics get added.\n",
    "    weightedData = normalizedData.mul([idW, opcodeW, locVarsCountW, tryCatchW, bNumberW, localityW])\n",
    "    newData = weightedData.join(pd.get_dummies(data[\"mutOperator\"]) * mutOpW)\n",
    "    newData = newData.join(pd.get_dummies(data[\"returnType\"]) * retTypeW)\n",
    "\n",
    "    return newData\n",
    "\n",
    "\n",
    "# Does not truly execute mutants, just calculates the precision doing so would cause.\n",
    "def execute_mutants(distances: np.ndarray, data: pd.DataFrame, centerSelection: int, preprocessing: int, seed: int) -> float:\n",
    "    df = pd.DataFrame()\n",
    "    df[\"id\"] = data[\"id\"]\n",
    "    df[\"killed\"] = data[\"killed\"]\n",
    "\n",
    "    # The cluster id the mutant is assigned is the index of the cluster center\n",
    "    # that has the lowest distance from the mutant.\n",
    "    df[\"cluster_id\"] = [np.argmin(nodeDistances) for nodeDistances in distances]\n",
    "    clusters = range(len(distances[0]))\n",
    "    correctLen = len(df)\n",
    "\n",
    "    # Assigns the killed/survived status to the other mutants in the cluster\n",
    "    # based on that of the cluster representative.\n",
    "    for cluster_id in clusters:\n",
    "        if centerSelection:\n",
    "            # Cluster representative = mutant with least distance where col index = cluster_id.\n",
    "            killed = df.iloc[np.argmin(distances[:, cluster_id])][\"killed\"]\n",
    "            df.loc[df[\"cluster_id\"] == cluster_id, \"prediction\"] = killed\n",
    "        else:\n",
    "            # Cluster representative = random from cluster.\n",
    "            tmp = df[df[\"cluster_id\"] == cluster_id]\n",
    "            killed = tmp.sample(random_state=seed).iloc[0][\"killed\"]\n",
    "            df.loc[df[\"cluster_id\"] == cluster_id, \"prediction\"] = killed\n",
    "\n",
    "    correctList = [1 for i, j in zip(df[\"killed\"].tolist(), df[\"prediction\"].tolist()) if i == j]\n",
    "    precision = 1/correctLen*len(correctList)\n",
    "    return precision\n",
    "\n",
    "\n",
    "# Calculates the precision for each reduction and for each seed if center selection is off.\n",
    "def calcPrecicions(data: pd.DataFrame, reductions: list, encoder: LabelEncoder, newData: pd.DataFrame, centerSelection: int, preprocess: int, seeds: list) -> list:\n",
    "    precisions = []\n",
    "    dataSave = data\n",
    "\n",
    "    for reduction in reductions:\n",
    "        data = dataSave.copy()\n",
    "        clustering = KMeans(n_clusters=int(math.ceil(len(data) * reduction)), n_init=1)\n",
    "\n",
    "        # This gives a NxM matrix of distances to cluster centers\n",
    "        # where N=# of Mutants and M=# of Clusters\n",
    "        clusters = clustering.fit_transform(newData)\n",
    "\n",
    "        # unlabel id so we can recognize the mutants\n",
    "        data[\"id\"] = encoder.inverse_transform(data[\"id\"])\n",
    "        if centerSelection:\n",
    "            precision = execute_mutants(clusters, data, centerSelection, preprocess, seeds)\n",
    "        else:\n",
    "            seedprecisions = []\n",
    "            for seed in seeds:\n",
    "                seedprecisions.append(execute_mutants(clusters, data, centerSelection, preprocess, seed))\n",
    "            precision = seedprecisions\n",
    "        precisions.append(precision)\n",
    "\n",
    "    return precisions\n",
    "\n",
    "\n",
    "def clusteredTestingSimulation(project: str, seeds: list, reductions: list, parameters: list, centerSelection: int=0, preprocess: int=0) -> list:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Gets characteristics and result status data from project file.\n",
    "    dataframes = getProjectDfs(project)\n",
    "    if dataframes == -1:\n",
    "        return -1\n",
    "\n",
    "    data, results = dataframes\n",
    "    data = data.merge(results, how=\"inner\", on=\"id\")\n",
    "\n",
    "    # define ordinal encoding\n",
    "    encoder = LabelEncoder()\n",
    "\n",
    "    # Uses Adam's rigorous preprocessing if preprocess.\n",
    "    # Original preprocessing otherwise.\n",
    "    if preprocess:\n",
    "        # Transform each column. Transform id last since we need to inverse that.\n",
    "        for col in [\"className\", \"methodName\", \"id\"]:\n",
    "            data[col] = encoder.fit_transform(data[col])\n",
    "        newData = preprocessing(data, parameters)\n",
    "    else:\n",
    "        # Transform each column.. do id last since we need to invert that.\n",
    "        for col in [\"mutOperator\", \"returnType\", \"className\", \"methodName\", \"id\"]:\n",
    "            data[col] = encoder.fit_transform(data[col])\n",
    "        newData = data.drop(columns=[\"killed\"])\n",
    "\n",
    "    return calcPrecicions(data, reductions, encoder, newData, centerSelection, preprocess, seeds)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "330fd7df",
   "metadata": {},
   "source": [
    "Parameter cell down below. Note that these are not the only parameters you might want to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544c776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [\n",
    "    66304, 16389, 14706, 91254, 49890, 86054, 55284, 77324, 36147, 13506, 73920, 80157, 43981, 75358, 33399, 56134,\n",
    "    13388, 81617, 90957, 52113, 20428, 26482, 56340, 31018, 32067, 13067, 8339, 49008, 125894, 68282, ]\n",
    "projects = [\"google-auto-common\", \"scribejava-core\", \"google-auto-factory\", \"commons-csv\",\n",
    "                \"commons-cli\", \"google-auto-value\", \"gson\", \"commons-io\",\"commons-text\", \"commons-codec\", ]\n",
    "reductions = [0.5, 0.25, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39bfff1b",
   "metadata": {},
   "source": [
    "The cell below calculates the precisions for every method, for every reduction, for every seed, for every project and prints it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23077e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "csImprovements = []\n",
    "ppImprovements = []\n",
    "csppImprovements = []\n",
    "\n",
    "for project in projects:\n",
    "    # Parameters used in Adam's thesis, picked from incomplete hyperopt run\n",
    "    parameters = [20491, 13, 44, 4584, 10188, 14214, 2443, 15598]\n",
    "    originalPrecisions = clusteredTestingSimulation(project, seeds, reductions, parameters)\n",
    "    if originalPrecisions == -1:\n",
    "        continue\n",
    "    csPrecisions = clusteredTestingSimulation(project, seeds, reductions, parameters, centerSelection=1)\n",
    "    ppPrecisions = clusteredTestingSimulation(project, seeds, reductions, parameters, centerSelection=0, preprocess=1)\n",
    "    csppPrecisions = clusteredTestingSimulation(project, seeds, reductions, parameters, centerSelection=1, preprocess=1)\n",
    "    print(\"Project = \" + project)\n",
    "    print(\"Original precisions = \" + str(originalPrecisions))\n",
    "    print(\"Precisions with center selection = \" + str(csPrecisions))\n",
    "    print(\"Precisions with preprocessing = \" + str(ppPrecisions))\n",
    "    print(\"Precisions with preprocessing and center selection = \" + str(csppPrecisions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ae4d424",
   "metadata": {},
   "source": [
    "We used this cell for our hyperopt runs. The variables you might want to change here are those inside the space, max_evals and the reductions(in the cell above).\n",
    "You may also want to work with less seeds/projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(args):\n",
    "#     parameters = [args[\"idW\"], args[\"mutOpW\"], args[\"opcodeW\"], args[\"retTypeW\"], args[\"locVarsCountW\"], args[\"tryCatchW\"], args[\"bNumberW\"], args[\"localityW\"]]\n",
    "#     print(parameters)\n",
    "#     precisions = []\n",
    "#\n",
    "#     for project in projects:\n",
    "#         meanPrec = np.mean(np.concatenate(clusteredTestingSimulation(project, seeds, reductions, parameters, preprocess=1)).flat)\n",
    "#         if meanPrec != -1:\n",
    "#             precisions.append(meanPrec)\n",
    "\n",
    "#     # Using np.hmean here instead could lead to more consistent performance across projects.\n",
    "#     print(\"Score = \" + str(100 - np.mean(precisions)))\n",
    "#     return 100 - np.mean(precisions)\n",
    "\n",
    "# space = {\"idW\": hp.randint(\"idW\", 5001, 15001),\n",
    "#          \"mutOpW\": hp.randint(\"mutOpW\", 1, 201),\n",
    "#          \"opcodeW\": hp.randint(\"opcodeW\", 1, 10001),\n",
    "#          \"retTypeW\": hp.randint(\"retTypeW\", 1, 10001),\n",
    "#          \"locVarsCountW\": hp.randint(\"locVarsCountW\", 8001, 15001),\n",
    "#          \"tryCatchW\":hp.randint(\"tryCatchW\", 8001, 15001),\n",
    "#          \"bNumberW\":hp.randint(\"bNumberW\", 1, 10001),\n",
    "#          \"localityW\": hp.randint(\"localityW\", 5001, 10001)}\n",
    "\n",
    "# best = fmin(objective, space, algo=tpe.suggest, max_evals=1)\n",
    "\n",
    "# with open(\"best.pkl\", 'wb') as outp:  # Overwrites any existing file.\n",
    "#     pickle.dump(best, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# print(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "c295b91a4a4f8e66f37da6a2fbf5c84e6919990d10548059361442497be2c972"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
