{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2c002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import pickle\n",
    "from os import path\n",
    "import os\n",
    "from random import sample\n",
    "import csv\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "# this is so we can render big dendogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d6ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_tests(project):\n",
    "    pitestOutput = subprocess.run([\"mvn\", \"test-compile\"], capture_output=True, cwd=\"projects/\" + project, text=True)\n",
    "    return pitestOutput\n",
    "\n",
    "def execute_normal(project):\n",
    "    pitestOutput = subprocess.run([\"mvn\", \"-Drat.skip=true\", \"org.pitest:pitest-maven:mutationCoverage\"], capture_output=True, cwd=\"projects/\" + project, text=True)\n",
    "    return pitestOutput\n",
    "\n",
    "def execute_verbose(project, addition=\"\"):\n",
    "    pitestOutput = subprocess.run([\"mvn\", \"-Dverbose=true\", \"-Drat.skip=true\", \"-Dfeatures=+cluster\", \"org.pitest:pitest-maven:mutationCoverage\"], capture_output=True, cwd=\"projects/\" + project, text=True)\n",
    "    with open(\"verboseOutputs/\" + project + addition  + \"-pitestOutput.txt\", \"w\") as verboseFile:\n",
    "        verboseFile.write(pitestOutput.stdout)\n",
    "    return pitestOutput.stdout.split(\"\\n\")\n",
    "\n",
    "def characteristics_extraction(project):\n",
    "    pitestOutput = subprocess.run([\"mvn\", \"-Drat.skip=true\", \"-Dfeatures=+characteristics\" \"org.pitest:pitest-maven:mutationCoverage\"], capture_output=True, cwd=\"projects/\" + project, text=True)\n",
    "    return pitestOutput\n",
    "\n",
    "def execute_cluster(project):\n",
    "    pitestOutput = subprocess.run([\"mvn\", \"-Drat.skip=true\", \"-Dfeatures=+cluster\", \"org.pitest:pitest-maven:mutationCoverage\"], capture_output=True, cwd=\"projects/\" + project, text=True)\n",
    "    return pitestOutput\n",
    "\n",
    "def read_verbose(project):\n",
    "    with open(\"verboseOutputs/\" + project + \"-pitestOutput.txt\") as verboseFile:\n",
    "        pitestOutput = verboseFile.readlines()\n",
    "        pitestOutput.append(\"filler\")\n",
    "        return pitestOutput\n",
    "\n",
    "def export_clusters(labels, csv_data, export_dir):\n",
    "    df = pd.DataFrame(columns=[\"id\", \"cluster_id\"])\n",
    "    for i in range(0, len(labels)):\n",
    "        df = df.append({\"id\": csv_data[\"id\"][i], \"cluster_id\": labels[i]}, ignore_index=True)\n",
    "\n",
    "    df.to_csv(export_dir + \"/clustering/clusters.csv\", sep=\",\", index=False)\n",
    "    return df\n",
    "\n",
    "# Source: tutorialspoint.com/file-searching-using-python\n",
    "def find_files(filename, search_path):\n",
    "   result = []\n",
    "\n",
    "    # Walking top-down from the root\n",
    "   for root, _, files in os.walk(search_path):\n",
    "      if filename in files:\n",
    "         result.append(os.path.join(root, filename))\n",
    "   return result\n",
    "\n",
    "def trainAlgorithm(projects, classificationAlgo=LogisticRegression, classAlgoParameters=[], parameters=[0.05, 5, 3, 3, 3, 3, 2, 2, 0.4]):\n",
    "    localityReduction, n_localVarsClusters, perOperator, perOpcode, perReturn, perTryCatch, perLocCluster, perVarsCluster, samplingStop = parameters\n",
    "    classificationTraining = []\n",
    "    for project in projects:\n",
    "        print(\"Starting training project: \" + project)\n",
    "        start_time = time.time()\n",
    "        csv_path = \"projects/\" + project\n",
    "        charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "        if charPath:\n",
    "            charPath = charPath[0]\n",
    "            data = pd.read_csv(charPath,\n",
    "                               names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                      \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                      \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                               skiprows=1)\n",
    "            killedPath = \"/\".join(charPath.split(\"/\")[:-1]) + \"/killed.csv\"\n",
    "            if path.exists(killedPath):\n",
    "                results = pd.read_csv(killedPath,\n",
    "                                      names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                      skiprows=1)\n",
    "                results = results.drop(columns=[\"numTests\"])\n",
    "            else:\n",
    "                print(\"Could not find killed.csv for project: \" + project)\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Could not find characteristics.csv for project: \" + project)\n",
    "            continue\n",
    "\n",
    "        # define ordinal encoding\n",
    "        encoder = LabelEncoder()\n",
    "        newData = data[[\"mutOperator\", \"opcode\", \"returnType\", \"localVarsCount\",\n",
    "                     \"isInTryCatch\", \"className\", \"methodName\", \"lineNumber\"]]\n",
    "        for col in [\"mutOperator\", \"returnType\", \"className\", \"methodName\"]:\n",
    "            newData[col] = encoder.fit_transform(newData[col])\n",
    "\n",
    "        # To-do: choose a better number than 100000\n",
    "        newData[\"className\"] = newData[\"className\"].apply(lambda x: x*100000)\n",
    "        # To-do: choose a better number than 1000\n",
    "        newData[\"methodName\"] = newData[\"methodName\"].apply(lambda x: x*1000)\n",
    "        localityClustering = AgglomerativeClustering(distance_threshold=None,\n",
    "                                                     n_clusters=int(math.ceil(len(data) * localityReduction)),\n",
    "                                                     linkage=\"ward\",\n",
    "                                                     compute_distances=False)\n",
    "        localityClustering.fit(newData[[\"className\", \"methodName\", \"lineNumber\"]])\n",
    "\n",
    "        varsClustering = AgglomerativeClustering(distance_threshold=None,\n",
    "                                                 n_clusters=n_localVarsClusters,\n",
    "                                                 linkage=\"ward\",\n",
    "                                                 compute_distances=False)\n",
    "        varsClustering.fit(newData[[\"localVarsCount\"]])\n",
    "\n",
    "        training = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\"isInTryCatch\"]]\n",
    "        training[\"localityCluster_id\"] = localityClustering.labels_\n",
    "        training[\"varsCluster_id\"] = varsClustering.labels_\n",
    "        training = training.merge(results, how=\"inner\", on=\"id\")\n",
    "\n",
    "        print(time.time() - start_time)\n",
    "        print(\"Starting creation of classification training set for project: \" + project)\n",
    "        start_time = time.time()\n",
    "\n",
    "        allMutants = []\n",
    "        mutantsSampleDfs = []\n",
    "        mutantsSample = []\n",
    "        i = 0\n",
    "        while True:\n",
    "            print(\"iteration = \", i)\n",
    "            i += 1\n",
    "            print(samplingStop * len(training[\"id\"].tolist()), len(allMutants))\n",
    "            mutantsSample = []\n",
    "            for mutOperator in list(dict.fromkeys(training[\"mutOperator\"].tolist())):\n",
    "                mutList = training[training[\"mutOperator\"] ==  mutOperator][\"id\"].tolist()\n",
    "                mutantsSample.extend(sample(mutList, min(len(mutList), perOperator)))\n",
    "            for opcode in list(dict.fromkeys(training[\"opcode\"].tolist())):\n",
    "                opList = training[training[\"mutOperator\"] ==  mutOperator][\"id\"].tolist()\n",
    "                mutantsSample.extend(sample(opList, min(len(opList), perOpcode)))\n",
    "            for returnType in list(dict.fromkeys(training[\"returnType\"].tolist())):\n",
    "                rTypeList = training[training[\"returnType\"] ==  returnType][\"id\"].tolist()\n",
    "                mutantsSample.extend(sample(rTypeList, min(len(rTypeList), perReturn)))\n",
    "            notInTCList = training[training[\"isInTryCatch\"] == 0][\"id\"].tolist()\n",
    "            mutantsSample.extend(sample(notInTCList, min(len(notInTCList), perTryCatch)))\n",
    "            inTCList = training[training[\"isInTryCatch\"] == 1][\"id\"].tolist()\n",
    "            mutantsSample.extend(sample(inTCList, min(len(inTCList), perTryCatch)))\n",
    "            for localityCluster_id in list(dict.fromkeys(training[\"localityCluster_id\"].tolist())):\n",
    "                locClusterList = training[training[\"localityCluster_id\"] == localityCluster_id][\"id\"].tolist()\n",
    "                mutantsSample.extend(sample(locClusterList, min(len(locClusterList), perLocCluster)))\n",
    "            for varsCluster_id in list(dict.fromkeys(training[\"varsCluster_id\"].tolist())):\n",
    "                varsClusterList = training[training[\"varsCluster_id\"] == varsCluster_id][\"id\"].tolist()\n",
    "                mutantsSample.extend(sample(varsClusterList, min(len(varsClusterList), perVarsCluster)))\n",
    "            allMutants.extend(mutantsSample)\n",
    "\n",
    "            trainingSample = training[training[\"id\"].isin(mutantsSample)]\n",
    "            for mutOperator in list(dict.fromkeys(trainingSample[\"mutOperator\"].tolist())):\n",
    "                percentageKilled = 1 / len(trainingSample[trainingSample[\"mutOperator\"] == mutOperator]) * len(trainingSample[(trainingSample[\"mutOperator\"] == mutOperator) & (trainingSample[\"killed\"] == 1)])\n",
    "                trainingSample.loc[trainingSample[\"mutOperator\"] == mutOperator, \"mutOperator\"] = percentageKilled\n",
    "            for opcode in list(dict.fromkeys(trainingSample[\"opcode\"].tolist())):\n",
    "                percentageKilled = 1 / len(trainingSample[trainingSample[\"opcode\"] == opcode]) * len(trainingSample[(trainingSample[\"opcode\"] == opcode) & (trainingSample[\"killed\"] == 1)])\n",
    "                trainingSample.loc[trainingSample[\"opcode\"] == opcode, \"opcode\"] = percentageKilled\n",
    "            for returnType in list(dict.fromkeys(trainingSample[\"returnType\"].tolist())):\n",
    "                percentageKilled = 1 / len(trainingSample[trainingSample[\"returnType\"] == returnType]) * len(trainingSample[(trainingSample[\"returnType\"] == returnType) & (trainingSample[\"killed\"] == 1)])\n",
    "                trainingSample.loc[trainingSample[\"returnType\"] == returnType, \"returnType\"] = percentageKilled\n",
    "            percentageKilled = 1 / len(trainingSample[trainingSample[\"isInTryCatch\"] == 0]) * len(trainingSample[(trainingSample[\"isInTryCatch\"] == 0) & (trainingSample[\"killed\"] == 1)])\n",
    "            trainingSample.loc[trainingSample[\"isInTryCatch\"] == 0, \"isInTryCatch\"] = percentageKilled\n",
    "            percentageKilled = 1 / len(trainingSample[trainingSample[\"isInTryCatch\"] == 1]) * len(trainingSample[(trainingSample[\"isInTryCatch\"] == 1) & (trainingSample[\"killed\"] == 1)])\n",
    "            trainingSample.loc[trainingSample[\"isInTryCatch\"] == 1, \"isInTryCatch\"] = percentageKilled\n",
    "            for localityCluster_id in list(dict.fromkeys(training[\"localityCluster_id\"].tolist())):\n",
    "                percentageKilled = 1 / len(trainingSample[trainingSample[\"localityCluster_id\"] == localityCluster_id]) * len(trainingSample[(trainingSample[\"localityCluster_id\"] == localityCluster_id) & (trainingSample[\"killed\"] == 1)])\n",
    "                trainingSample.loc[trainingSample[\"localityCluster_id\"] == localityCluster_id, \"localityCluster_id\"] = percentageKilled\n",
    "            for varsCluster_id in list(dict.fromkeys(training[\"varsCluster_id\"].tolist())):\n",
    "                percentageKilled = 1 / len(trainingSample[trainingSample[\"varsCluster_id\"] == varsCluster_id]) * len(trainingSample[(trainingSample[\"varsCluster_id\"] == varsCluster_id) & (trainingSample[\"killed\"] == 1)])\n",
    "                trainingSample.loc[trainingSample[\"varsCluster_id\"] == varsCluster_id, \"varsCluster_id\"] = percentageKilled\n",
    "\n",
    "            mutantsSampleDfs.append(trainingSample)\n",
    "#             if samplingStop * len(training[\"id\"].tolist()) > len(set(mutantsSample) & set(allMutants)):\n",
    "#                 break\n",
    "            if samplingStop * len(training[\"id\"].tolist()) < len(allMutants):\n",
    "                break\n",
    "\n",
    "        classificationTraining.append(pd.concat(mutantsSampleDfs).drop(columns=[\"id\"]))\n",
    "\n",
    "    print(\"Done with getting all classification training data.\")\n",
    "    print(time.time() - start_time)\n",
    "\n",
    "    classificationTrainingDf = pd.concat(classificationTraining)\n",
    "    X_train = classificationTrainingDf.values.tolist()\n",
    "    y_train = classificationTrainingDf[\"killed\"].tolist()\n",
    "\n",
    "    print(\"Starting training of algorithm.\")\n",
    "    start_time = time.time()\n",
    "    classifier = classificationAlgo(*classAlgoParameters)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    print(time.time() - start_time)\n",
    "    return classifier\n",
    "\n",
    "def own_predict(classifier, project, parameters=[0.05, 5, 3, 3, 3, 3, 2, 2, 0.01]):\n",
    "    localityReduction, n_localVarsClusters, perOperator, perOpcode, perReturn, perTryCatch, perLocCluster, perVarsCluster, samplingStop = parameters\n",
    "    print(\"Starting prediction project: \" + project)\n",
    "    start_time = time.time()\n",
    "    csv_path = \"projects/\" + project\n",
    "    charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "    if charPath:\n",
    "        charPath = charPath[0]\n",
    "        data = pd.read_csv(charPath,\n",
    "                           names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                    \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                    \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                           skiprows=1)\n",
    "        # Normally 1 would need to execute during prediction instead of checking the file for results.\n",
    "        killedPath = \"/\".join(charPath.split(\"/\")[:-1]) + \"/killed.csv\"\n",
    "        if path.exists(killedPath):\n",
    "            results = pd.read_csv(killedPath,\n",
    "                                  names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                  skiprows=1)\n",
    "            results = results.drop(columns=[\"numTests\"])\n",
    "        else:\n",
    "            print(\"Could not find killed.csv for project: \" + project)\n",
    "            return 1\n",
    "    else:\n",
    "        print(\"Could not find characteristics.csv for project: \" + project)\n",
    "        return 1\n",
    "\n",
    "    # define ordinal encoding\n",
    "    encoder = LabelEncoder()\n",
    "    newData = data[[\"mutOperator\", \"opcode\", \"returnType\", \"localVarsCount\",\n",
    "                    \"isInTryCatch\",\"className\", \"methodName\", \"lineNumber\"]]\n",
    "    for col in [\"mutOperator\", \"returnType\", \"className\", \"methodName\"]:\n",
    "        newData[col] = encoder.fit_transform(newData[col])\n",
    "\n",
    "    # To-do: choose a better number than 100000\n",
    "    newData[\"className\"] = newData[\"className\"].apply(lambda x: x*100000)\n",
    "    # To-do: choose a better number than 1000\n",
    "    newData[\"methodName\"] = newData[\"methodName\"].apply(lambda x: x*1000)\n",
    "    localityClustering = AgglomerativeClustering(distance_threshold=None,\n",
    "                                                 n_clusters=int(math.ceil(len(data) * localityReduction)),\n",
    "                                                 linkage=\"ward\",\n",
    "                                                 compute_distances=False)\n",
    "    localityClustering.fit(newData[[\"className\", \"methodName\", \"lineNumber\"]])\n",
    "\n",
    "    varsClustering = AgglomerativeClustering(distance_threshold=None, n_clusters=n_localVarsClusters, linkage=\"ward\", compute_distances=False)\n",
    "    varsClustering.fit(newData[[\"localVarsCount\"]])\n",
    "\n",
    "    training = data[[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\"isInTryCatch\"]]\n",
    "    training[\"localityCluster_id\"] = localityClustering.labels_\n",
    "    training[\"varsCluster_id\"] = varsClustering.labels_\n",
    "    training = training.merge(results, how=\"inner\", on=\"id\")\n",
    "\n",
    "    mutantsSample = []\n",
    "    for mutOperator in list(dict.fromkeys(training[\"mutOperator\"].tolist())):\n",
    "        mutList = training[training[\"mutOperator\"] ==  mutOperator][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(mutList, min(len(mutList), perOperator)))\n",
    "    for opcode in list(dict.fromkeys(training[\"opcode\"].tolist())):\n",
    "        opList = training[training[\"mutOperator\"] ==  mutOperator][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(opList, min(len(opList), perOpcode)))\n",
    "    for returnType in list(dict.fromkeys(training[\"returnType\"].tolist())):\n",
    "        rTypeList = training[training[\"returnType\"] ==  returnType][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(rTypeList, min(len(rTypeList), perReturn)))\n",
    "    notInTCList = training[training[\"isInTryCatch\"] == 0][\"id\"].tolist()\n",
    "    mutantsSample.extend(sample(notInTCList, min(len(notInTCList), perTryCatch)))\n",
    "    inTCList = training[training[\"isInTryCatch\"] == 1][\"id\"].tolist()\n",
    "    mutantsSample.extend(sample(inTCList, min(len(inTCList), perTryCatch)))\n",
    "    for localityCluster_id in list(dict.fromkeys(training[\"localityCluster_id\"].tolist())):\n",
    "        locClusterList = training[training[\"localityCluster_id\"] == localityCluster_id][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(locClusterList, min(len(locClusterList), perLocCluster)))\n",
    "    for varsCluster_id in list(dict.fromkeys(training[\"varsCluster_id\"].tolist())):\n",
    "        varsClusterList = training[training[\"varsCluster_id\"] == varsCluster_id][\"id\"].tolist()\n",
    "        mutantsSample.extend(sample(varsClusterList, min(len(varsClusterList), perVarsCluster)))\n",
    "\n",
    "    trainingSample = training[training[\"id\"].isin(mutantsSample)]\n",
    "    print(\"reduction = \" + str(1 / len(training) * len(trainingSample)))\n",
    "    for mutOperator in list(dict.fromkeys(trainingSample[\"mutOperator\"].tolist())):\n",
    "        percentageKilled = 1 / len(trainingSample[trainingSample[\"mutOperator\"] == mutOperator]) * len(trainingSample[(trainingSample[\"mutOperator\"] == mutOperator) & (trainingSample[\"killed\"] == 1)])\n",
    "        training.loc[training[\"mutOperator\"] == mutOperator, \"mutOperator\"] = percentageKilled\n",
    "    for opcode in list(dict.fromkeys(trainingSample[\"opcode\"].tolist())):\n",
    "        percentageKilled = 1 / len(trainingSample[trainingSample[\"opcode\"] == opcode]) * len(trainingSample[(trainingSample[\"opcode\"] == opcode) & (trainingSample[\"killed\"] == 1)])\n",
    "        training.loc[training[\"opcode\"] == opcode, \"opcode\"] = percentageKilled\n",
    "    for returnType in list(dict.fromkeys(trainingSample[\"returnType\"].tolist())):\n",
    "        percentageKilled = 1 / len(trainingSample[trainingSample[\"returnType\"] == returnType]) * len(trainingSample[(trainingSample[\"returnType\"] == returnType) & (trainingSample[\"killed\"] == 1)])\n",
    "        training.loc[training[\"returnType\"] == returnType, \"returnType\"] = percentageKilled\n",
    "    percentageKilled = 1 / len(trainingSample[trainingSample[\"isInTryCatch\"] == 0]) * len(trainingSample[(trainingSample[\"isInTryCatch\"] == 0) & (trainingSample[\"killed\"] == 1)])\n",
    "    training.loc[training[\"isInTryCatch\"] == 0, \"isInTryCatch\"] = percentageKilled\n",
    "    percentageKilled = 1 / len(trainingSample[trainingSample[\"isInTryCatch\"] == 1]) * len(trainingSample[(trainingSample[\"isInTryCatch\"] == 1) & (trainingSample[\"killed\"] == 1)])\n",
    "    training.loc[training[\"isInTryCatch\"] == 1, \"isInTryCatch\"] = percentageKilled\n",
    "    for localityCluster_id in list(dict.fromkeys(training[\"localityCluster_id\"].tolist())):\n",
    "        percentageKilled = 1 / len(trainingSample[trainingSample[\"localityCluster_id\"] == localityCluster_id]) * len(trainingSample[(trainingSample[\"localityCluster_id\"] == localityCluster_id) & (trainingSample[\"killed\"] == 1)])\n",
    "        training.loc[training[\"localityCluster_id\"] == localityCluster_id, \"localityCluster_id\"] = percentageKilled\n",
    "    for varsCluster_id in list(dict.fromkeys(training[\"varsCluster_id\"].tolist())):\n",
    "        percentageKilled = 1 / len(trainingSample[trainingSample[\"varsCluster_id\"] == varsCluster_id]) * len(trainingSample[(trainingSample[\"varsCluster_id\"] == varsCluster_id) & (trainingSample[\"killed\"] == 1)])\n",
    "        training.loc[training[\"varsCluster_id\"] == varsCluster_id, \"varsCluster_id\"] = percentageKilled\n",
    "\n",
    "    training = training.drop(columns=[\"id\"])\n",
    "    predictResults = classifier.predict(training)\n",
    "    print(\"Prediction for project \" + project + \" took: \" + str(time.time() - start_time) + \" seconds.\")\n",
    "    return predictResults\n",
    "\n",
    "def precisionCalc(project, predictions):\n",
    "    csv_path = \"projects/\" + project\n",
    "    charPath = find_files(\"characteristics.csv\", csv_path)\n",
    "    if charPath:\n",
    "        charPath = charPath[0]\n",
    "        data = pd.read_csv(charPath,\n",
    "                           names=[\"id\", \"mutOperator\", \"opcode\", \"returnType\",\n",
    "                                    \"localVarsCount\", \"isInTryCatch\", \"isInFinalBlock\",\n",
    "                                    \"className\", \"methodName\", \"blockNumber\", \"lineNumber\"],\n",
    "                           skiprows=1)\n",
    "        # Normally 1 would need to execute during prediction instead of checking the file for results.\n",
    "        killedPath = \"/\".join(charPath.split(\"/\")[:-1]) + \"/killed.csv\"\n",
    "        if path.exists(killedPath):\n",
    "            results = pd.read_csv(killedPath,\n",
    "                                  names=[\"id\", \"killed\", \"numTests\"],\n",
    "                                  skiprows=1)\n",
    "            results = results.drop(columns=[\"numTests\"])\n",
    "        else:\n",
    "            print(\"Could not find killed.csv for project: \" + project)\n",
    "            return 1\n",
    "    else:\n",
    "        print(\"Could not find characteristics.csv for project: \" + project)\n",
    "        return 1\n",
    "    data[\"prediction\"] = predictions\n",
    "    newData = data[[\"id\", \"prediction\"]]\n",
    "    merged = newData.merge(results, how=\"inner\", on=\"id\")\n",
    "    merged.to_csv(\"projects/\" + project + \"/predictions.csv\", sep=\",\", index=False)\n",
    "    correctList = [1 if i == j else 0 for i, j in zip(merged[\"killed\"].tolist(), merged[\"prediction\"].tolist())]\n",
    "    print(1/len(correctList)*len([1 for val in correctList if val == 1 ]))\n",
    "    print([idx for idx, val in enumerate(correctList) if val == 0])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "544c776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped = [\"zxing\", \"commons-lang\", \"jodatime\", \"jfreechart\", ]\n",
    "# projects = [\"google-auto-service\", \"google-auto-common\", \"scribejava-core\", \"google-auto-factory\", \"commons-csv\",\n",
    "#                 \"commons-cli\", \"google-auto-value\", \"gson\", \"commons-io\",\"commons-text\", \"commonc-codec\", ]\n",
    "# projects1 = [\"commons-text\", \"commonc-codec\", ]\n",
    "seeds = [\n",
    "    66304, 16389, 14706, 91254, 49890, 86054, 55284, 77324, 36147, 13506, 73920, 80157, 43981, 75358, 33399, 56134,\n",
    "    13388, 81617, 90957, 52113, 20428, 26482, 56340, 31018, 32067, 13067, 8339, 49008, 125894, 68282, ]\n",
    "trainingProjects = [\"google-auto-service\", \"scribejava-core\", \"commons-cli\",\n",
    "                    \"google-auto-value\", ]\n",
    "projects = [ \"google-auto-factory\", \"google-auto-common\", \"commons-csv\",\"commons-io\",\"commons-text\", \"commons-codec\",]\n",
    "unused = [\"gson\", \"commons-io\",\"commons-text\", \"commons-codec\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23077e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "Starting training project: google-auto-service\n",
      "0.025596141815185547\n",
      "Starting creation of classification training set for project: google-auto-service\n",
      "iteration =  0\n",
      "190.0 0\n",
      "Starting training project: scribejava-core\n",
      "0.9699885845184326\n",
      "Starting creation of classification training set for project: scribejava-core\n",
      "iteration =  0\n",
      "2298.4 0\n",
      "iteration =  1\n",
      "2298.4 767\n",
      "iteration =  2\n",
      "2298.4 1534\n",
      "Starting training project: commons-cli\n",
      "1.3798832893371582\n",
      "Starting creation of classification training set for project: commons-cli\n",
      "iteration =  0\n",
      "2876.0 0\n",
      "iteration =  1\n",
      "2876.0 999\n",
      "iteration =  2\n",
      "2876.0 1998\n",
      "Starting training project: google-auto-value\n",
      "7.928802728652954\n",
      "Starting creation of classification training set for project: google-auto-value\n",
      "iteration =  0\n",
      "6698.400000000001 0\n",
      "iteration =  1\n",
      "6698.400000000001 1899\n",
      "iteration =  2\n",
      "6698.400000000001 3798\n",
      "iteration =  3\n",
      "6698.400000000001 5697\n",
      "Done with getting all classification training data.\n",
      "4.677155256271362\n",
      "Starting training of algorithm.\n",
      "0.0823967456817627\n",
      "Starting predictions\n",
      "Starting prediction project: google-auto-factory\n",
      "reduction = 0.12294238683127572\n",
      "Prediction for project google-auto-factory took: 1.3853192329406738 seconds.\n",
      "Results for project: google-auto-factory\n",
      "0.9999999999999999\n",
      "[]\n",
      "Starting prediction project: google-auto-common\n",
      "reduction = 0.1283770837325158\n",
      "Prediction for project google-auto-common took: 1.1010749340057373 seconds.\n",
      "Results for project: google-auto-common\n",
      "0.9961678482467906\n",
      "[865, 866, 867, 871, 872, 1352, 1353, 1354, 1356, 1357, 1358, 1359, 1360, 2233, 2234, 2235, 2238, 2239, 4865, 4866]\n",
      "Starting prediction project: commons-csv\n",
      "reduction = 0.12337098175499565\n",
      "Prediction for project commons-csv took: 1.7222678661346436 seconds.\n",
      "Results for project: commons-csv\n",
      "0.9999999999999999\n",
      "[]\n",
      "Starting prediction project: commons-io\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "print(\"Starting training\")\n",
    "classifier = trainAlgorithm(trainingProjects)\n",
    "with open(\"classifier.pkl\", 'wb') as outp:  # Overwrites any existing file.\n",
    "    pickle.dump(classifier, outp, pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Starting predictions\")\n",
    "for project in projects:\n",
    "    results = own_predict(classifier, project)\n",
    "    print(\"Results for project: \" + project)\n",
    "    precisionCalc(project, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2b4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
